{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from time import time\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split  # Needed to split the data\n",
    "from Historic_Crypto import HistoricalData\n",
    "from Historic_Crypto import Cryptocurrencies\n",
    "from Historic_Crypto import LiveCryptoData\n",
    "import keras as keras\n",
    "import yfinance as yf # Needed to utilize yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.1.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "        self.d1 = tf.keras.layers.Dense(256)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.f1 = tf.keras.layers.Flatten()\n",
    "        self.d2 = tf.keras.layers.Dense(512)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.d3 = tf.keras.layers.Dense(512)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop2 = tf.keras.layers.Dropout(0.3)\n",
    "        self.d4 = tf.keras.layers.Dense(256)\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop3 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dv1 = tf.keras.layers.Dense(128)  # value hidden layer\n",
    "        self.da1 = tf.keras.layers.Dense(128)  # actions hidden layer\n",
    "        self.dv2 = tf.keras.layers.Dense(1, activation=None)  # value output\n",
    "        self.da2 = tf.keras.layers.Dense(9, activation=None)  # actions output\n",
    "\n",
    "    def call(self, input_data):\n",
    "        x = self.leaky_relu(self.d1(input_data))\n",
    "        x = self.bn1(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.leaky_relu(self.d2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.leaky_relu(self.d3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.leaky_relu(self.d4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.drop3(x)\n",
    "        v = self.leaky_relu(self.dv1(x))\n",
    "        a = self.leaky_relu(self.da1(x))\n",
    "        v = self.dv2(v)\n",
    "        a = self.da2(a)\n",
    "        Q = v + (a - tf.math.reduce_mean(a, axis=1, keepdims=True))\n",
    "        return Q\n",
    "\n",
    "    def advantage(self, state):\n",
    "        x = self.leaky_relu(self.d1(state))\n",
    "        x = self.bn1(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.leaky_relu(self.d2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.leaky_relu(self.d3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.leaky_relu(self.d4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.drop3(x)\n",
    "        a = self.leaky_relu(self.da1(x))\n",
    "        a = self.da2(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpReplay():\n",
    "    def __init__(self, num_features, window_size, buffer_size = 1000000):\n",
    "        self.num_features = num_features\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_mem = np.zeros((self.buffer_size, self.num_features, window_size), dtype=np.float32)\n",
    "        # ones because we use these as indices later when selecting an action from a list\n",
    "        self.action_mem = np.ones((self.buffer_size), dtype=np.int32)  \n",
    "        self.reward_mem = np.zeros((self.buffer_size), dtype=np.float32)\n",
    "        self.next_state_mem = np.zeros((self.buffer_size, self.num_features, window_size), dtype=np.float32)\n",
    "        # we want to make sure we are aware if we sample a terminal memory to zero out future expected rewards for this time step.\n",
    "        self.done_mem = np.zeros((self.buffer_size), dtype=bool) \n",
    "        self.counter = 0\n",
    "\n",
    "    def add_exp(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add the results of an action to the memory. Record the state before the action, the action we took to get there, the reward after the action, the next state, and whether the action ended the game.\n",
    "        \"\"\"\n",
    "        # by using mod operator, we ensure we overwrite old data when the buffer is full. (1000001 % 1000000 = 1)\n",
    "        pointer = self.counter % self.buffer_size\n",
    "        self.state_mem[pointer] = state\n",
    "        self.action_mem[pointer] = action\n",
    "        self.reward_mem[pointer] = reward\n",
    "        self.next_state_mem[pointer] = next_state\n",
    "        # done is a bool, where 1 is True and 0 is false. We subtract it from 1 here because we multiply it by future rewards to zero-out future rewards in terminal states\n",
    "        self.done_mem[pointer] = 1 - int(done)\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample_exp(self, batch_size=64):\n",
    "        max_mem = min(self.counter, self.buffer_size)  # get the amount of filled memory or total memory size, whichever is less.\n",
    "        # get a list (batch) of random indices between 0 and the number of filled spots in memory (max_mem). batch_size shows that we want a list of 64 random indices. replace=false means that we can't sample the same memory index twice.\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        states = self.state_mem[batch] # sample 64 states from memory using the random indices we saved in \"batch\"\n",
    "        actions = self.action_mem[batch]\n",
    "        rewards = self.reward_mem[batch]\n",
    "        next_states = self.next_state_mem[batch]\n",
    "        dones = self.done_mem[batch]\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, data_shape, num_episodes, window_size=4, gamma=0.99, update_interval=96, lr=0.01, min_epsilon=0.02):\n",
    "        self.window_size = window_size\n",
    "        self.data_shape = data_shape\n",
    "        self.portfolio = [0, 0]  # [total eth, cash_held]\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = 1.0\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.update_interval = update_interval\n",
    "        self.trainstep = 0\n",
    "        self.memory = ExpReplay(self.window_size, data_shape[1])\n",
    "        self.batch_size = 64\n",
    "        self.online_net = DDDQN()\n",
    "        self.target_net = DDDQN()\n",
    "\n",
    "        initial_learning_rate = lr\n",
    "        decay_steps = self.num_episodes * self.data_shape[0] // 10  # You can adjust the divisor to control the decay rate\n",
    "        decay_rate = 0.9  # You can adjust this value to control the decay rate\n",
    "        lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.online_net.compile(loss='mse', optimizer=opt)\n",
    "        self.target_net.compile(loss='mse', optimizer=opt) # we won't ever optimize this network for loss, since we are only copying weights from our online network every 100 steps\n",
    "\n",
    "    def get_action(self, state, cash_balance):\n",
    "        # create a random number between 0 and 1 and compare it to epsilon for our epsilon-greedy algorithm\n",
    "        if tf.random.uniform([]) <= self.epsilon:\n",
    "            # Select a random action out of the list of available actions\n",
    "            if self.portfolio[0] > 0.01:  # if we have portfolio to sell\n",
    "                return np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "            elif cash_balance > 0:  # if we have money to buy\n",
    "                return np.random.choice([4, 5, 6, 7, 8])\n",
    "            else:  # if we have neither\n",
    "                action = 4 # hold\n",
    "                return action\n",
    "        else:\n",
    "            # Select the action with the highest Q-value\n",
    "            actions = self.online_net.advantage(np.array([state]))[0]\n",
    "            if self.portfolio[0] > 0.01:\n",
    "                action = tf.math.argmax(actions, axis=0).numpy()\n",
    "            elif cash_balance > 0:\n",
    "                action = tf.math.argmax(actions[4:], axis=0).numpy() + 4\n",
    "            else:\n",
    "                action = 4  # hold action\n",
    "            return action\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.set_weights(self.online_net.get_weights()) # copies the weights of our target DQN from our online DQN (copies the network)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        # if self.epsilon > self.min_epsilon:\n",
    "        #     b = self.min_epsilon**(1/(self.num_episodes*self.data_shape[0]))\n",
    "        #     self.epsilon = b**self.trainstep\n",
    "        epsilon_decay = (1.0 - self.min_epsilon) / (self.num_episodes * self.data_shape[0])\n",
    "        self.epsilon = max(self.epsilon - epsilon_decay, self.min_epsilon)\n",
    "\n",
    "    def train(self):\n",
    "        # if we haven't filled our batch of 64 with enough experiences to fill our memory yet, we don't train our model. This way we don't train our model every single action, just every 64.\n",
    "        if self.memory.counter < self.batch_size: \n",
    "            return \n",
    "        \n",
    "        # every 100 steps, we update our target network. the 100 is set in the update_interval variable. This prevents us form \"chasing our own tail\", and improves the stability of our Deep Q Learning.\n",
    "        if self.trainstep % self.update_interval == 0: \n",
    "            self.update_target()\n",
    "        \n",
    "        # Here we use experience replay buffers to sample our memory of past experiences.\n",
    "        # In contrast to consuming samples online and discarding them thereafter, sampling from the stored experiences means they are less heavily correlated and can be re-used for learning.\n",
    "        # Can we improve this with Prioritized Experience Replay? It improves by weighing the importance of different memories differently.\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size) # we sample 64 experiences from our memory since we are training in batches of 64 and not every time\n",
    "        \n",
    "        # feed-forward:\n",
    "        # We calculate the possible Q values with our online network, then we calculate again the Q values with our target network and find which action has the highest Q value. (this is where double DQN comes in)\n",
    "        # We then select the action with the best Q value determined by our online network, and use the Q value from the corresponding action in our target network to train our online network (minimizing loss).\n",
    "        q_next_state_online_net = self.online_net.predict(next_states, verbose=0)  # equivalent to __call__()\n",
    "        q_next_state_target_net = self.target_net.predict(next_states, verbose=0)\n",
    "        \n",
    "        max_action = tf.math.argmax(q_next_state_online_net, axis=1, output_type=tf.dtypes.int32).numpy()\n",
    "        # Now that we know which action provided the maximum Q value according to our online network, we can train our online model using the Q value from the corresponding action in our target network.\n",
    "        # 1. We predict the Q values for the action we actually took using our online network and save the values in q_predicted\n",
    "        # 2. we copy q_predicted to q_target because of how Keras/TF calculates loss (we have to copy it to a fresh variable). By using np.copy, we zero out all losses associated with actions not taken  \n",
    "        # 3. We update the Q values of q_target with the rewards we got from the action we actually took and the future value according to our target network (gamma, our discount rate, time the next step Q value)\n",
    "        # 4. We update the weights of our online network by training it using this ^\n",
    "\n",
    "        batch_index = tf.range(self.batch_size, dtype=tf.dtypes.int32)\n",
    "\n",
    "        q_predicted = self.online_net.predict(states, verbose=0) # in our equation for the optimal Q function, we need to know the value of the action we actually took.\n",
    "        q_target = np.copy(q_predicted) # We copy q_predicted to q_target because of how Keras/TF calculates loss. By using np.copy, we basically zero out all losses associated with actions not taken \n",
    "\n",
    "        # The Q value for each state in our batch, for the action we ACTUALLY took, is equal to the reward for that time step plus gamma times the Q value of the next step, according to our target network.\n",
    "        # this q_target is the Q value we are chasing after with our estimates. It is the estimated \"optimal\" Q value based on the Bellman equation.\n",
    "\n",
    "        # print(\"actions.shape:\", actions.shape)\n",
    "        # print(\"batch_index.shape:\", batch_index.shape)\n",
    "        # print(\"max_action.shape:\", max_action.shape)\n",
    "        q_target[batch_index, actions] = rewards + self.gamma * q_next_state_target_net[batch_index, max_action] * dones # if a state is done, done has been set to 0. This is because the expected future rewards for a terminal state is always 0.\n",
    "                # ^ we subset our q_target array (remember: this is actually a copy of our predictions from the online network) by selecting 64 entries according to batch_index, then the best action according to actions.\n",
    "                # it returns a 1d array of 64 elements with the best action per memory entry in the batch, which we replace with the reward and expected outcome according to our target network\n",
    "\n",
    "        loss = self.online_net.train_on_batch(states, q_target)\n",
    "        self.update_epsilon()\n",
    "        self.trainstep += 1\n",
    "        return loss\n",
    "\n",
    "    def save_model(self):\n",
    "        self.online_net.save(\"Output/online_model.h5\")\n",
    "        self.target_net.save(\"Output/target_model.h5\")\n",
    "\n",
    "    def calculate_reward(self, t, eth_df, eth_close_price_unscaled, amount_to_sell, initial_investment, trading_fee_rate, risk_adjustment):\n",
    "                        ether_held, cash_held = self.portfolio\n",
    "                        \n",
    "                        scaled_close_price = eth_df[\"Close\"].iloc[t]\n",
    "                        unscaled_close_price = eth_close_price_unscaled[\"Close\"].iloc[t]\n",
    "                        value_of_eth_sold = unscaled_close_price * amount_to_sell\n",
    "                        trading_fee = value_of_eth_sold * trading_fee_rate\n",
    "                        cash_received = value_of_eth_sold - trading_fee\n",
    "                        new_cash_held = cash_held + cash_received\n",
    "                        new_ether_held = ether_held - amount_to_sell\n",
    "                        \n",
    "                        # Calculate portfolio value\n",
    "                        portfolio_value = new_cash_held + (new_ether_held * unscaled_close_price) - initial_investment\n",
    "                        \n",
    "                        # Calculate risk-adjusted return\n",
    "                        asset_weights = [new_cash_held / portfolio_value, new_ether_held * unscaled_close_price / portfolio_value]\n",
    "                        asset_volatility = [eth_close_price_unscaled[\"Close\"].pct_change().rolling(window=252).std().iloc[t] for asset in asset_weights]\n",
    "                        portfolio_volatility = np.dot(asset_weights, asset_volatility)\n",
    "                        risk_adjusted_reward = (portfolio_value - cash_held) / (portfolio_volatility * risk_adjustment)\n",
    "                        \n",
    "                        # Add some reward for selling if a sell flag was set?\n",
    "                        \n",
    "                        # Update portfolio\n",
    "                        self.portfolio = [new_ether_held, new_cash_held]\n",
    "\n",
    "                        return risk_adjusted_reward\n",
    "\n",
    "    def trade(self, t, action, eth_df, eth_df_unscaled, initial_investment, trading_fee_rate, risk_adjustment):\n",
    "        reward = 0\n",
    "        eth_held, cash_balance = self.portfolio\n",
    "        sell_percentages = [0.25, 0.5, 0.75, 1.0]\n",
    "        buy_percentages = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "        if action >= 0 and action <= 3 and eth_held > 0.01:\n",
    "            # print(\"Selling: \" + str(sell_percentages[action]) + \"% of portfolio at eth price \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "            # print(\"Current cash_balance: \" + str(round(cash_balance, 2)))\n",
    "            sell_percentage = sell_percentages[action]\n",
    "            amount_to_sell = eth_held * sell_percentage\n",
    "\n",
    "            while amount_to_sell > 0 and self.portfolio[0] > 0.01:\n",
    "                scaled_item_cost = eth_df[\"Close\"].iloc[t]\n",
    "                unscaled_item_cost = eth_df_unscaled[\"Close\"].iloc[t]\n",
    "                # print(\"Eth amount: \" + str(round(eth_held, 7)))\n",
    "                # print(\"Current ask price: \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "                # print('----------------')\n",
    "\n",
    "                reward = self.calculate_reward(t, eth_df, eth_df_unscaled, amount_to_sell, initial_investment, trading_fee_rate, risk_adjustment)\n",
    "\n",
    "                eth_held, cash_balance = self.portfolio # Update portfolio after selling\n",
    "\n",
    "                # print(\"Amount Eth sold: \" + str(amount_to_sell))\n",
    "                # print(\"Selling price: \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "                # print(\"Trading fee: \" + str(trading_fee_rate * unscaled_item_cost))\n",
    "                # print(\"New Eth amount: \" + str(round(eth_held, 7)))\n",
    "                # print(\"New cash_balance: \" + str(round(cash_balance, 2)))\n",
    "                amount_to_sell = 0\n",
    "        elif action == 4:\n",
    "            # print(\"Hold: Price is \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "            reward = -0.01\n",
    "        elif action >= 5 and cash_balance >= 0:\n",
    "            # print(\"Buy Ether with: \" + str(buy_percentages[action - 5]) + \"% of cash_balance at Eth price \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "            buy_percentage = buy_percentages[action - 5]\n",
    "            eth_to_buy = (cash_balance * buy_percentage) / eth_df_unscaled[\"Close\"].iloc[t]\n",
    "            # print(\"Current cash_balance: \" + str(round(cash_balance, 2)))\n",
    "            # print(\"Current Eth amount: \" + str(round(eth_held, 7)))\n",
    "            # print(\"Eth purchased: \" + str(eth_to_buy))\n",
    "            self.portfolio[0] += eth_to_buy\n",
    "            self.portfolio[1] -= eth_df_unscaled[\"Close\"].iloc[t] * eth_to_buy\n",
    "            # print(\"New cash_balance: \" + str(round(self.portfolio[1], 2)))\n",
    "            # add some reward for buying if a buy flag is set?\n",
    "        return reward\n",
    "    \n",
    "    def get_state(self, t, eth_df):\n",
    "        num_rows = t - self.window_size + 1\n",
    "        if num_rows >= 0:\n",
    "            window = eth_df.iloc[num_rows : t + 1]\n",
    "        else:\n",
    "            repeated_first_row = pd.concat([pd.DataFrame(np.repeat(eth_df.iloc[[0]].values, -num_rows, axis=0), columns=eth_df.columns)])\n",
    "            new_data = eth_df.iloc[0 : t + 1]\n",
    "            window = pd.concat([repeated_first_row, new_data], ignore_index=True)  # prevents us from sampling data that doesn't exist at the start.\n",
    "        return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crypto_data(start_date, end_date, split_date):\n",
    "    try:\n",
    "        train = pd.read_csv(\"Datasets/train_eth.csv\", index_col=0)\n",
    "        test = pd.read_csv(\"Datasets/test_eth.csv\", index_col=0)\n",
    "    except:\n",
    "        df = HistoricalData('ETH-USD', 3600, start_date, end_date).retrieve_data()  # 3600 is the interval in seconds which is 1 hour\n",
    "        split_row = df.index.get_loc(split_date)\n",
    "        cols = df.columns.tolist()\n",
    "        df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=cols)\n",
    "        train = df[:split_row]\n",
    "        test = df[split_row:]\n",
    "        train.to_csv(\"Datasets/train_eth.csv\")\n",
    "        test.to_csv(\"Datasets/test_eth.csv\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crypto_data(start_date, end_date):\n",
    "    historic_data_df = HistoricalData('ETH-USD', 21600, start_date, end_date).retrieve_data()  # 3600 is the interval in seconds which is 1 hour\n",
    "    cols = historic_data_df.columns.tolist()\n",
    "    historic_data_df = pd.DataFrame(MinMaxScaler().fit_transform(historic_data_df), columns=cols)\n",
    "    historic_data_df.to_csv(\"Datasets/train_eth.csv\")\n",
    "    return historic_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-collected data\n",
    "def get_precollected_data(split_ratio=0.8):\n",
    "    # try:\n",
    "    historic_data = pd.read_csv(\"Datasets/eth_historical_data.csv\", index_col=0)\n",
    "    print('Pre-collected data loaded successfully.')\n",
    "    # convert date to datetime if it isn't the index\n",
    "    if historic_data.index.name != 'Date':\n",
    "        historic_data['Date'] = pd.to_datetime(historic_data['Date'], infer_datetime_format=True)\n",
    "        print('Date converted to datetime.')\n",
    "        # set date as index\n",
    "        historic_data = historic_data.set_index('Date')\n",
    "        print('Date set as index.')\n",
    "    # drop nan values\n",
    "    historic_data = historic_data.dropna()\n",
    "    print('NaN values dropped.')\n",
    "    cols = historic_data.columns.tolist()\n",
    "    train_unscaled, test_unscaled = train_test_split(historic_data, train_size=split_ratio,shuffle=False)\n",
    "    train_scaled = pd.DataFrame(MinMaxScaler().fit_transform(train_unscaled), columns=cols)\n",
    "    test_scaled = pd.DataFrame(MinMaxScaler().fit_transform(test_unscaled), columns=cols)\n",
    "    print('Data split.')\n",
    "    # set the index to the date\n",
    "    train_unscaled = pd.DataFrame(train_unscaled,  columns=cols)\n",
    "    test_unscaled = pd.DataFrame(test_unscaled,  columns=cols)\n",
    "    # convert train_unscaled and test_unscaled to df with only the Close column\n",
    "    train_unscaled = train_unscaled[['Close']]\n",
    "    test_unscaled = test_unscaled[['Close']]\n",
    "\n",
    "    train_unscaled.to_csv(\"Output/train_unscaled.csv\")\n",
    "    train_scaled.to_csv(\"Output/train_scaled.csv\")\n",
    "    print('Train data saved.')\n",
    "    test_unscaled.to_csv(\"Output/test_unscaled.csv\")\n",
    "    test_scaled.to_csv(\"Output/test_scaled.csv\")\n",
    "    print('Test data saved.')\n",
    "    return train_scaled, test_scaled, train_unscaled, test_unscaled\n",
    "        \n",
    "    # except:\n",
    "    #     # throw error\n",
    "    #     print(\"No pre-collected data found. Please run the data collection script first.\")\n",
    "    #     exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 82\n"
     ]
    }
   ],
   "source": [
    "historic_data = pd.read_csv(\"Datasets/eth_historical_data.csv\", index_col=0)\n",
    "\n",
    "historic_data.head()\n",
    "\n",
    "# print num of columns\n",
    "print('Number of columns: ' + str(len(historic_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_INVESTMENT = 1000\n",
    "NUM_EPISODES = 30\n",
    "START_DATE = \"2017-01-01-00-00\"\n",
    "END_DATE = \"2020-01-01-00-00\"\n",
    "TESTING_SPLIT = \"2022-01-01-00:00:00\" # formatting is different here because of how historic_crypto indexes dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-collected data loaded successfully.\n",
      "NaN values dropped.\n",
      "Data split.\n",
      "Train data saved.\n",
      "Test data saved.\n",
      "\n",
      "===== Episode 1 / 30 =====\n",
      "Time step 0 / 956   |  Eth Held: 1.1295176  |  Cash Held: 750.0  |  Portfolio Value (USD): 1000.0   |   Realized Profit: -250.0   |   Unrealized Profit: 0.0   |  Epsilon: 1.0   |   MSE Loss: 0.0\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Time step 100 / 956   |  Eth Held: 1.2219299  |  Cash Held: 213.87  |  Portfolio Value (USD): 439.006   |   Realized Profit: -786.125   |   Unrealized Profit: -560.994   |  Epsilon: 0.999   |   MSE Loss: nan\n",
      "Time step 200 / 956   |  Eth Held: 0.7316527  |  Cash Held: 43.85  |  Portfolio Value (USD): 182.313   |   Realized Profit: -956.153   |   Unrealized Profit: -817.687   |  Epsilon: 0.995   |   MSE Loss: nan\n",
      "Time step 300 / 956   |  Eth Held: 0.3823092  |  Cash Held: 24.57  |  Portfolio Value (USD): 102.159   |   Realized Profit: -975.43   |   Unrealized Profit: -897.841   |  Epsilon: 0.992   |   MSE Loss: nan\n",
      "Time step 400 / 956   |  Eth Held: 0.0404875  |  Cash Held: 48.08  |  Portfolio Value (USD): 64.105   |   Realized Profit: -951.921   |   Unrealized Profit: -935.895   |  Epsilon: 0.988   |   MSE Loss: nan\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # train_df, test_df = get_crypto_data(START_DATE, END_DATE, TESTING_SPLIT)\n",
    "    train_df, test_df, train_close, test_close = get_precollected_data(split_ratio=0.8)\n",
    "    trading_agent = Agent(train_df.shape, NUM_EPISODES, window_size=48, gamma=0.95, update_interval=96, lr=0.01, min_epsilon=0.02)\n",
    "    episode_mem = [{\"Actions\": [], \"Eth Held\": [], \"Cash Held\": [], \"Portfolio Value (USD)\": [], \"Realized Profit\": [], \"Unrealized Profit\": [], \"Reward\": [], \"Done\": [], \"Epsilon\": [], \"MSE Loss\": []} for i in range(NUM_EPISODES)]\n",
    "    t0 = time()\n",
    "\n",
    "    ######################## Training ########################\n",
    "    for s in range(NUM_EPISODES):\n",
    "        print(f\"\\n===== Episode {s + 1} / {NUM_EPISODES} =====\")\n",
    "        state = trading_agent.get_state(0, train_df)\n",
    "        cash_balance = INITIAL_INVESTMENT\n",
    "        # Reset the agent's portfolio at the beginning of each episode\n",
    "        trading_agent.portfolio = [0 , INITIAL_INVESTMENT]\n",
    "\n",
    "        done = False\n",
    "        for t in range(len(train_df) - 1):\n",
    "            if done:\n",
    "                break\n",
    "            action = trading_agent.get_action(state, cash_balance)\n",
    "            next_state = trading_agent.get_state(t + 1, train_df)\n",
    "            reward = trading_agent.trade(t, action, train_df, train_close, INITIAL_INVESTMENT, trading_fee_rate = 0.05, risk_adjustment=0.1)\n",
    "            eth_held, cash_held = trading_agent.portfolio\n",
    "            cash_balance = cash_held  # update cash balance\n",
    "            if t != 0:  # if not the first trade\n",
    "                done = cash_balance <= 1 and eth_held <= 0.01\n",
    "            trading_agent.memory.add_exp(state, action, reward, next_state, done)\n",
    "            loss = trading_agent.train()\n",
    "            if not loss:\n",
    "                loss = 0\n",
    "            state = next_state\n",
    "            \n",
    "            episode_mem[s][\"Actions\"].append(int(action))\n",
    "            episode_mem[s][\"Eth Held\"].append(float(eth_held))\n",
    "            episode_mem[s][\"Cash Held\"].append(round(float(cash_balance), 2))\n",
    "            episode_mem[s][\"Portfolio Value (USD)\"].append(float(train_close[\"Close\"].iloc[t] * eth_held + cash_balance))\n",
    "            episode_mem[s][\"Realized Profit\"].append(float(cash_balance - INITIAL_INVESTMENT))\n",
    "            episode_mem[s][\"Unrealized Profit\"].append(float(train_close[\"Close\"].iloc[t] * eth_held + cash_balance - INITIAL_INVESTMENT))\n",
    "            episode_mem[s][\"Reward\"].append(float(reward))\n",
    "            episode_mem[s][\"Done\"].append(bool(done))\n",
    "            episode_mem[s][\"Epsilon\"].append(trading_agent.epsilon)\n",
    "            episode_mem[s][\"MSE Loss\"].append(float(loss))\n",
    "\n",
    "            if t % 100 == 0:\n",
    "                print(f\"Time step {t} / {len(train_df)}   |  Eth Held: {round(episode_mem[s]['Eth Held'][t], 7)}  |  Cash Held: {round(episode_mem[s]['Cash Held'][t], 2)}  |  Portfolio Value (USD): {round(episode_mem[s]['Portfolio Value (USD)'][t], 3)}   |   Realized Profit: {round(episode_mem[s]['Realized Profit'][t], 3)}   |   Unrealized Profit: {round(episode_mem[s]['Unrealized Profit'][t], 3)}   |  Epsilon: {round(trading_agent.epsilon, 3)}   |   MSE Loss: {round(episode_mem[s]['MSE Loss'][t], 3)}\")\n",
    "\n",
    "        with open('Output/training_scores.out', 'a') as f:\n",
    "            f.write(f\"EPISODE {s} (runtime: {time() - t0})   |   Unrealized Profit is {round(episode_mem[s]['Unrealized Profit'][-1], 3)}   |  Portfolio Value (USD) is {round(episode_mem[s]['Portfolio Value (USD)'][-1], 3)} Epsilon is {round(trading_agent.epsilon, 3)}   |   MSE Loss is {round(episode_mem[s]['MSE Loss'][-1], 3)}\\n\")\n",
    "\n",
    "    with open('Output/episode_mem.json', 'w') as f:\n",
    "        json.dump(episode_mem, f)\n",
    "\n",
    "    ######################## Testing ########################\n",
    "    t0 = time()\n",
    "    testing_mem = {\"Actions\": [], \"Eth Held\": [], \"Cash Held\": [], \"Portfolio Value (USD)\": [], \"Realized Profit\": [], \"Unrealized Profit\": [], \"Reward\": [], \"Done\": []}\n",
    "    trading_agent.epsilon = 0\n",
    "    state = trading_agent.get_state(0, test_df)\n",
    "    cash_balance = INITIAL_INVESTMENT\n",
    "    # Reset the agent's portfolio at the beginning of each episode\n",
    "    trading_agent.portfolio = [0 , INITIAL_INVESTMENT]\n",
    "\n",
    "    done = False\n",
    "    for t in range(len(test_df) - 1):\n",
    "        if done:\n",
    "            break\n",
    "        action = trading_agent.get_action(state, cash_balance)\n",
    "        next_state = trading_agent.get_state(t + 1, test_df)\n",
    "        reward = trading_agent.trade(t, action, test_df, test_close, INITIAL_INVESTMENT, trading_fee_rate = 0.05, risk_adjustment=0.1)\n",
    "        eth_held, cash_held  = trading_agent.portfolio\n",
    "        cash_balance = cash_held  # update cash balance\n",
    "        done = cash_balance <= 1 and eth_held <= 0.01\n",
    "        state = next_state\n",
    "\n",
    "        testing_mem[\"Actions\"].append(int(action))\n",
    "        testing_mem[\"Eth Held\"].append(float(eth_held))\n",
    "        testing_mem[\"Cash Held\"].append(round(float(cash_balance),2))\n",
    "        testing_mem[\"Portfolio Value (USD)\"].append(float(test_close[\"Close\"].iloc[t] * eth_held + cash_balance))\n",
    "        testing_mem[\"Realized Profit\"].append(float(cash_balance - INITIAL_INVESTMENT))\n",
    "        testing_mem[\"Unrealized Profit\"].append(float(test_close[\"Close\"].iloc[t] * eth_held + cash_balance - INITIAL_INVESTMENT))\n",
    "        testing_mem[\"Reward\"].append(float(reward))\n",
    "        testing_mem[\"Done\"].append(bool(done))\n",
    "\n",
    "        if t % 1 == 0:\n",
    "            print(f\"Time step {t} / {len(test_df)}   |   Eth Held: {round(testing_mem['Eth Held'][t], 7)}  |  Cash Held: {round(testing_mem['Cash Held'][t], 2)}  |  Portfolio Value (USD): {round(testing_mem['Portfolio Value (USD)'][t], 3)}   |   Realized Profit: {round(testing_mem['Realized Profit'][t], 3)}   |   Unrealized Profit: {round(testing_mem['Unrealized Profit'][t], 3)}\")\n",
    "\n",
    "    with open('Output/testing_scores.out', 'a') as f:\n",
    "        f.write(f\"TESTING (runtime: {time() - t0})   |   Unrealized Profit is {round(testing_mem['Unrealized Profit'][-1], 3)}   |  Portfolio Value (USD) is {round(testing_mem['Portfolio Value (USD)'][-1], 3)}\\n\")\n",
    "\n",
    "    with open('Output/testing_mem.json', 'w') as f:\n",
    "        json.dump(testing_mem, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df, test_df, train_close, test_close = get_precollected_data(split_ratio=0.8)\n",
    "\n",
    "def plot_data(data, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_df[\"Close\"], label=\"Ethereum Price\")\n",
    "    plt.plot(data[\"Portfolio Value (USD)\"], label=\"Portfolio Value (USD)\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "# visualize training data\n",
    "with open('Output/episode_mem.json', 'r') as f:\n",
    "    episode_mem = json.load(f)\n",
    "\n",
    "with open(\"Output/testing_mem.json\", \"r\") as f:\n",
    "    testing_mem = json.load(f)\n",
    "\n",
    "plot_data(episode_mem[29], \"Training Data\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot((testing_mem[\"Portfolio Value (USD)\"]))\n",
    "plt.plot(test_df['Close'])\n",
    "plt.legend([\"Ethereum Price\", \"Average Portfolio Value (USD)\"])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()\n",
    "# plot ethereum price history\n",
    "# set plot width and height\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(historic_data[\"Close\"])\n",
    "plt.title(\"Ethereum Price History\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()\n",
    "\n",
    "# # plot average portfolio value\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Portfolio Value (USD)\"])\n",
    "# plt.legend([\"Ethereum Price\", \"Average Portfolio Value (USD)\"])\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"Price\")\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Portfolio Value (USD)\"])\n",
    "# plt.title(\"Portfolio Value (USD)\")\n",
    "# plt.show()\n",
    "\n",
    "for i in range(NUM_EPISODES):\n",
    "    plt.plot(episode_mem[i][\"Realized Profit\"])\n",
    "plt.title(\"Realized Profit\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(episode_mem[0][\"Realized Profit\"])\n",
    "plt.title(\"Realized Profit\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(episode_mem[29][\"Realized Profit\"])\n",
    "plt.title(\"Realized Profit\")\n",
    "plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Reward\"])\n",
    "# plt.title(\"Reward\")\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Epsilon\"])\n",
    "# plt.title(\"Epsilon\")\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"MSE Loss\"])\n",
    "# plt.title(\"MSE Loss\")\n",
    "# plt.show()\n",
    "\n",
    "# # visualize testing data\n",
    "# with open('Output/testing_mem.json', 'r') as f:\n",
    "#     testing_mem = json.load(f)\n",
    "\n",
    "# # plot portfolio value vs ethereum price\n",
    "# plt.plot(testing_mem[\"Portfolio Value (USD)\"])\n",
    "# plt.title(\"Portfolio Value (USD)\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(testing_mem[\"Realized Profit\"])\n",
    "# plt.title(\"Realized Profit\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(testing_mem[\"Reward\"])\n",
    "# plt.title(\"Reward\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning_20220719",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23361560846584cf1ea895074b40a7ea878b24065d1c129dab67edcdc8abadf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
