{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from time import time\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split  # Needed to split the data\n",
    "from Historic_Crypto import HistoricalData\n",
    "from Historic_Crypto import Cryptocurrencies\n",
    "from Historic_Crypto import LiveCryptoData\n",
    "import keras as keras\n",
    "import yfinance as yf # Needed to utilize yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.1.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "        self.d1 = tf.keras.layers.Dense(256)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.f1 = tf.keras.layers.Flatten()\n",
    "        self.d2 = tf.keras.layers.Dense(512)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.d3 = tf.keras.layers.Dense(512)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop2 = tf.keras.layers.Dropout(0.3)\n",
    "        self.d4 = tf.keras.layers.Dense(256)\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop3 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dv1 = tf.keras.layers.Dense(128)  # value hidden layer\n",
    "        self.da1 = tf.keras.layers.Dense(128)  # actions hidden layer\n",
    "        self.dv2 = tf.keras.layers.Dense(1, activation=None)  # value output\n",
    "        self.da2 = tf.keras.layers.Dense(9, activation=None)  # actions output\n",
    "\n",
    "    def call(self, input_data):\n",
    "        x = self.leaky_relu(self.d1(input_data))\n",
    "        x = self.bn1(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.leaky_relu(self.d2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.leaky_relu(self.d3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.leaky_relu(self.d4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.drop3(x)\n",
    "        v = self.leaky_relu(self.dv1(x))\n",
    "        a = self.leaky_relu(self.da1(x))\n",
    "        v = self.dv2(v)\n",
    "        a = self.da2(a)\n",
    "        Q = v + (a - tf.math.reduce_mean(a, axis=1, keepdims=True))\n",
    "        return Q\n",
    "\n",
    "    def advantage(self, state):\n",
    "        x = self.leaky_relu(self.d1(state))\n",
    "        x = self.bn1(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.leaky_relu(self.d2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.leaky_relu(self.d3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.leaky_relu(self.d4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.drop3(x)\n",
    "        a = self.leaky_relu(self.da1(x))\n",
    "        a = self.da2(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpReplay():\n",
    "    def __init__(self, num_features, window_size, buffer_size = 1000000):\n",
    "        self.num_features = num_features\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_mem = np.zeros((self.buffer_size, self.num_features, window_size), dtype=np.float32)\n",
    "        # ones because we use these as indices later when selecting an action from a list\n",
    "        self.action_mem = np.ones((self.buffer_size), dtype=np.int32)  \n",
    "        self.reward_mem = np.zeros((self.buffer_size), dtype=np.float32)\n",
    "        self.next_state_mem = np.zeros((self.buffer_size, self.num_features, window_size), dtype=np.float32)\n",
    "        # we want to make sure we are aware if we sample a terminal memory to zero out future expected rewards for this time step.\n",
    "        self.done_mem = np.zeros((self.buffer_size), dtype=bool) \n",
    "        self.counter = 0\n",
    "\n",
    "    def add_exp(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add the results of an action to the memory. Record the state before the action, the action we took to get there, the reward after the action, the next state, and whether the action ended the game.\n",
    "        \"\"\"\n",
    "        # by using mod operator, we ensure we overwrite old data when the buffer is full. (1000001 % 1000000 = 1)\n",
    "        pointer = self.counter % self.buffer_size\n",
    "        self.state_mem[pointer] = state\n",
    "        self.action_mem[pointer] = action\n",
    "        self.reward_mem[pointer] = reward\n",
    "        self.next_state_mem[pointer] = next_state\n",
    "        # done is a bool, where 1 is True and 0 is false. We subtract it from 1 here because we multiply it by future rewards to zero-out future rewards in terminal states\n",
    "        self.done_mem[pointer] = 1 - int(done)\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample_exp(self, batch_size=64):\n",
    "        max_mem = min(self.counter, self.buffer_size)  # get the amount of filled memory or total memory size, whichever is less.\n",
    "        # get a list (batch) of random indices between 0 and the number of filled spots in memory (max_mem). batch_size shows that we want a list of 64 random indices. replace=false means that we can't sample the same memory index twice.\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        states = self.state_mem[batch] # sample 64 states from memory using the random indices we saved in \"batch\"\n",
    "        actions = self.action_mem[batch]\n",
    "        rewards = self.reward_mem[batch]\n",
    "        next_states = self.next_state_mem[batch]\n",
    "        dones = self.done_mem[batch]\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, data_shape, num_episodes, window_size=4, gamma=0.99, update_interval=96, lr=0.01, min_epsilon=0.02):\n",
    "        self.window_size = window_size\n",
    "        self.data_shape = data_shape\n",
    "        self.portfolio = [0, 0, 0]  # [total eth, cash_held, total_portfolio_value (eth value + cash held - initial investment)]\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = 1.0\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.update_interval = update_interval\n",
    "        self.trainstep = 0\n",
    "        self.memory = ExpReplay(self.window_size, data_shape[1])\n",
    "        self.batch_size = 64\n",
    "        self.online_net = DDDQN()\n",
    "        self.target_net = DDDQN()\n",
    "\n",
    "        initial_learning_rate = lr\n",
    "        decay_steps = self.num_episodes * self.data_shape[0] // 10  # You can adjust the divisor to control the decay rate\n",
    "        decay_rate = 0.9  # You can adjust this value to control the decay rate\n",
    "        lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.online_net.compile(loss='mse', optimizer=opt)\n",
    "        self.target_net.compile(loss='mse', optimizer=opt) # we won't ever optimize this network for loss, since we are only copying weights from our online network every 100 steps\n",
    "\n",
    "    def get_action(self, state, cash_balance):\n",
    "        # create a random number between 0 and 1 and compare it to epsilon for our epsilon-greedy algorithm\n",
    "        if tf.random.uniform([]) <= self.epsilon:\n",
    "            # Select a random action out of the list of available actions\n",
    "            if self.portfolio[0] > 0.01:  # if we have portfolio to sell\n",
    "                return np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "            elif cash_balance > 0:  # if we have money to buy\n",
    "                return np.random.choice([4, 5, 6, 7, 8])\n",
    "            else:  # if we have neither\n",
    "                action = 4 # hold\n",
    "                return action\n",
    "        else:\n",
    "            # Select the action with the highest Q-value\n",
    "            actions = self.online_net.advantage(np.array([state]))[0]\n",
    "            if self.portfolio[0] > 0.01:\n",
    "                action = tf.math.argmax(actions, axis=0).numpy()\n",
    "            elif cash_balance > 0:\n",
    "                action = tf.math.argmax(actions[4:], axis=0).numpy() + 4\n",
    "            else:\n",
    "                action = 4  # hold action\n",
    "            return action\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.set_weights(self.online_net.get_weights()) # copies the weights of our target DQN from our online DQN (copies the network)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            b = self.min_epsilon**(1/(self.num_episodes*self.data_shape[0]))\n",
    "            self.epsilon = b**self.trainstep\n",
    "        # epsilon_decay = (1.0 - self.min_epsilon) / (self.num_episodes * self.data_shape[0])\n",
    "        # self.epsilon = max(self.epsilon - epsilon_decay, self.min_epsilon)\n",
    "\n",
    "    def train(self):\n",
    "        # if we haven't filled our batch of 64 with enough experiences to fill our memory yet, we don't train our model. This way we don't train our model every single action, just every 64.\n",
    "        if self.memory.counter < self.batch_size: \n",
    "            return \n",
    "        \n",
    "        # every 100 steps, we update our target network. the 100 is set in the update_interval variable. This prevents us form \"chasing our own tail\", and improves the stability of our Deep Q Learning.\n",
    "        if self.trainstep % self.update_interval == 0: \n",
    "            self.update_target()\n",
    "        \n",
    "        # Here we use experience replay buffers to sample our memory of past experiences.\n",
    "        # In contrast to consuming samples online and discarding them thereafter, sampling from the stored experiences means they are less heavily correlated and can be re-used for learning.\n",
    "        # Can we improve this with Prioritized Experience Replay? It improves by weighing the importance of different memories differently.\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size) # we sample 64 experiences from our memory since we are training in batches of 64 and not every time\n",
    "\n",
    "        # feed-forward:\n",
    "        # We calculate the possible Q values with our online network, then we calculate again the Q values with our target network and find which action has the highest Q value. (this is where double DQN comes in)\n",
    "        # We then select the action with the best Q value determined by our online network, and use the Q value from the corresponding action in our target network to train our online network (minimizing loss).\n",
    "        q_next_state_online_net = self.online_net.predict(next_states, verbose=0)  # equivalent to __call__()\n",
    "        q_next_state_target_net = self.target_net.predict(next_states, verbose=0)\n",
    "\n",
    "        max_action = tf.math.argmax(q_next_state_online_net, axis=1, output_type=tf.dtypes.int32).numpy()\n",
    "        # Now that we know which action provided the maximum Q value according to our online network, we can train our online model using the Q value from the corresponding action in our target network.\n",
    "        # 1. We predict the Q values for the action we actually took using our online network and save the values in q_predicted\n",
    "        # 2. we copy q_predicted to q_target because of how Keras/TF calculates loss (we have to copy it to a fresh variable). By using np.copy, we zero out all losses associated with actions not taken  \n",
    "        # 3. We update the Q values of q_target with the rewards we got from the action we actually took and the future value according to our target network (gamma, our discount rate, time the next step Q value)\n",
    "        # 4. We update the weights of our online network by training it using this ^\n",
    "\n",
    "        batch_index = tf.range(self.batch_size, dtype=tf.dtypes.int32)\n",
    "\n",
    "        q_predicted = self.online_net.predict(states, verbose=0) # in our equation for the optimal Q function, we need to know the value of the action we actually took.\n",
    "        q_target = np.copy(q_predicted) # We copy q_predicted to q_target because of how Keras/TF calculates loss. By using np.copy, we basically zero out all losses associated with actions not taken \n",
    "\n",
    "        # The Q value for each state in our batch, for the action we ACTUALLY took, is equal to the reward for that time step plus gamma times the Q value of the next step, according to our target network.\n",
    "        # this q_target is the Q value we are chasing after with our estimates. It is the estimated \"optimal\" Q value based on the Bellman equation.\n",
    "\n",
    "        # print(\"actions.shape:\", actions.shape)\n",
    "        # print(\"batch_index.shape:\", batch_index.shape)\n",
    "        # print(\"max_action.shape:\", max_action.shape)\n",
    "        q_target[batch_index, actions] = rewards + self.gamma * q_next_state_target_net[batch_index, max_action] * dones # if a state is done, done has been set to 0. This is because the expected future rewards for a terminal state is always 0.\n",
    "                # ^ we subset our q_target array (remember: this is actually a copy of our predictions from the online network) by selecting 64 entries according to batch_index, then the best action according to actions.\n",
    "                # it returns a 1d array of 64 elements with the best action per memory entry in the batch, which we replace with the reward and expected outcome according to our target network\n",
    "\n",
    "        loss = self.online_net.train_on_batch(states, q_target)\n",
    "        self.update_epsilon()\n",
    "        self.trainstep += 1\n",
    "        return loss\n",
    "\n",
    "    def save_model(self):\n",
    "        import os\n",
    "\n",
    "        output_directory = \"Output\"\n",
    "        online_model_directory = os.path.join(output_directory, \"online_model\")\n",
    "        target_model_directory = os.path.join(output_directory, \"target_model\")\n",
    "\n",
    "        os.makedirs(online_model_directory, exist_ok=True)\n",
    "        os.makedirs(target_model_directory, exist_ok=True)\n",
    "\n",
    "        self.online_net.save(online_model_directory, save_format=\"tf\")\n",
    "        self.target_net.save(target_model_directory, save_format=\"tf\")\n",
    "\n",
    "\n",
    "    def calculate_reward(self, t, eth_df, eth_close_price_unscaled, amount_to_sell, initial_investment, trading_fee_rate):\n",
    "        ether_held, cash_held, previous_portfolio_value = self.portfolio\n",
    "        \n",
    "        unscaled_close_price = eth_close_price_unscaled[\"Close\"].iloc[t]\n",
    "        value_of_eth_sold = unscaled_close_price * amount_to_sell\n",
    "        trading_fee = value_of_eth_sold * trading_fee_rate\n",
    "        cash_received = value_of_eth_sold - trading_fee\n",
    "        new_cash_held = cash_held + cash_received\n",
    "        new_ether_held = ether_held - amount_to_sell\n",
    "        \n",
    "        # Calculate portfolio value\n",
    "        new_portfolio_value = new_cash_held + (new_ether_held * unscaled_close_price) - initial_investment\n",
    "        \n",
    "        # Calculate the reward based on the change in portfolio value\n",
    "        reward = new_portfolio_value - previous_portfolio_value\n",
    "\n",
    "        # Update portfolio\n",
    "        self.portfolio = [new_ether_held, new_cash_held, new_portfolio_value]\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def trade(self, t, action, eth_df, eth_df_unscaled, initial_investment, trading_fee_rate):\n",
    "        reward = 0\n",
    "        eth_held, cash_balance, previous_portfolio_value = self.portfolio\n",
    "        sell_percentages = [0.25, 0.5, 0.75, 1.0]\n",
    "        buy_percentages = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "        if action >= 0 and action <= 3 and eth_held > 0.01:\n",
    "            # print(\"Selling: \" + str(sell_percentages[action]) + \"% of portfolio at eth price \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "            # print(\"Current cash_balance: \" + str(round(cash_balance, 2)))\n",
    "            sell_percentage = sell_percentages[action]\n",
    "            amount_to_sell = eth_held * sell_percentage\n",
    "\n",
    "            while amount_to_sell > 0 and self.portfolio[0] > 0.01:\n",
    "                scaled_item_cost = eth_df[\"Close\"].iloc[t]\n",
    "                unscaled_item_cost = eth_df_unscaled[\"Close\"].iloc[t]\n",
    "                # print(\"Eth amount: \" + str(round(eth_held, 7)))\n",
    "                # print(\"Current ask price: \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "                # print('----------------')\n",
    "\n",
    "                reward = self.calculate_reward(t, eth_df, eth_df_unscaled, amount_to_sell, initial_investment, trading_fee_rate)\n",
    "\n",
    "                eth_held, cash_balance, current_portfolio_value = self.portfolio # Update portfolio after selling\n",
    "\n",
    "                # print(\"Amount Eth sold: \" + str(amount_to_sell))\n",
    "                # print(\"Selling price: \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "                # print(\"Trading fee: \" + str(trading_fee_rate * unscaled_item_cost))\n",
    "                # print(\"New Eth amount: \" + str(round(eth_held, 7)))\n",
    "                # print(\"New cash_balance: \" + str(round(cash_balance, 2)))\n",
    "                amount_to_sell = 0\n",
    "        elif action == 4:\n",
    "            # print(\"Hold: Price is \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "            self.portfolio[2] = cash_balance + (eth_held * eth_df_unscaled[\"Close\"].iloc[t]) - initial_investment\n",
    "            reward = -0.01\n",
    "        elif action >= 5 and cash_balance >= 0:\n",
    "            # print(\"Buy Ether with: \" + str(buy_percentages[action - 5]) + \"% of cash_balance at Eth price \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "            buy_percentage = buy_percentages[action - 5]\n",
    "            eth_to_buy = (cash_balance * buy_percentage) / eth_df_unscaled[\"Close\"].iloc[t]\n",
    "            # print(\"Current cash_balance: \" + str(round(cash_balance, 2)))\n",
    "            # print(\"Current Eth amount: \" + str(round(eth_held, 7)))\n",
    "            # print(\"Eth purchased: \" + str(eth_to_buy))\n",
    "            self.portfolio[0] += eth_to_buy\n",
    "            self.portfolio[1] -= eth_df_unscaled[\"Close\"].iloc[t] * eth_to_buy\n",
    "            self.portfolio[2] = self.portfolio[1] + (self.portfolio[0] * eth_df_unscaled[\"Close\"].iloc[t]) - initial_investment\n",
    "            # print(\"New cash_balance: \" + str(round(self.portfolio[1], 2)))\n",
    "            # add some reward for buying if a buy flag is set?\n",
    "        return reward\n",
    "    \n",
    "    def get_state(self, t, eth_df):\n",
    "        num_rows = t - self.window_size + 1\n",
    "        if num_rows >= 0:\n",
    "            window = eth_df.iloc[num_rows : t + 1]\n",
    "        else:\n",
    "            repeated_first_row = pd.concat([pd.DataFrame(np.repeat(eth_df.iloc[[0]].values, -num_rows, axis=0), columns=eth_df.columns)])\n",
    "            new_data = eth_df.iloc[0 : t + 1]\n",
    "            window = pd.concat([repeated_first_row, new_data], ignore_index=True)  # prevents us from sampling data that doesn't exist at the start.\n",
    "        return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crypto_data(start_date, end_date, split_date):\n",
    "    try:\n",
    "        train = pd.read_csv(\"Datasets/train_eth.csv\", index_col=0)\n",
    "        test = pd.read_csv(\"Datasets/test_eth.csv\", index_col=0)\n",
    "    except:\n",
    "        df = HistoricalData('ETH-USD', 3600, start_date, end_date).retrieve_data()  # 3600 is the interval in seconds which is 1 hour\n",
    "        split_row = df.index.get_loc(split_date)\n",
    "        cols = df.columns.tolist()\n",
    "        df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=cols)\n",
    "        train = df[:split_row]\n",
    "        test = df[split_row:]\n",
    "        train.to_csv(\"Datasets/train_eth.csv\")\n",
    "        test.to_csv(\"Datasets/test_eth.csv\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crypto_data(start_date, end_date):\n",
    "    historic_data_df = HistoricalData('ETH-USD', 21600, start_date, end_date).retrieve_data()  # 3600 is the interval in seconds which is 1 hour\n",
    "    cols = historic_data_df.columns.tolist()\n",
    "    historic_data_df = pd.DataFrame(MinMaxScaler().fit_transform(historic_data_df), columns=cols)\n",
    "    historic_data_df.to_csv(\"Datasets/train_eth.csv\")\n",
    "    return historic_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-collected data\n",
    "def get_precollected_data(split_ratio=0.8):\n",
    "    # try:\n",
    "    historic_data = pd.read_csv(\"Datasets/eth_historical_data_simple.csv\", index_col=0)\n",
    "    print('Pre-collected data loaded successfully.')\n",
    "    # convert date to datetime if it isn't the index\n",
    "    if historic_data.index.name != 'Date':\n",
    "        historic_data['Date'] = pd.to_datetime(historic_data['Date'], infer_datetime_format=True)\n",
    "        print('Date converted to datetime.')\n",
    "        # set date as index\n",
    "        historic_data = historic_data.set_index('Date')\n",
    "        print('Date set as index.')\n",
    "    # drop nan values\n",
    "    historic_data = historic_data.dropna()\n",
    "    print('NaN values dropped.')\n",
    "    cols = historic_data.columns.tolist()\n",
    "    train_unscaled, test_unscaled = train_test_split(historic_data, train_size=split_ratio,shuffle=False)\n",
    "    train_scaled = pd.DataFrame(MinMaxScaler().fit_transform(train_unscaled), columns=cols)\n",
    "    test_scaled = pd.DataFrame(MinMaxScaler().fit_transform(test_unscaled), columns=cols)\n",
    "    print('Data split.')\n",
    "    # set the index to the date\n",
    "    train_unscaled = pd.DataFrame(train_unscaled,  columns=cols)\n",
    "    test_unscaled = pd.DataFrame(test_unscaled,  columns=cols)\n",
    "    # convert train_unscaled and test_unscaled to df with only the Close column\n",
    "    train_unscaled = train_unscaled[['Close']]\n",
    "    test_unscaled = test_unscaled[['Close']]\n",
    "\n",
    "    train_unscaled.to_csv(\"Output/train_unscaled.csv\")\n",
    "    train_scaled.to_csv(\"Output/train_scaled.csv\")\n",
    "    print('Train data saved.')\n",
    "    test_unscaled.to_csv(\"Output/test_unscaled.csv\")\n",
    "    test_scaled.to_csv(\"Output/test_scaled.csv\")\n",
    "    print('Test data saved.')\n",
    "    return train_scaled, test_scaled, train_unscaled, test_unscaled\n",
    "        \n",
    "    # except:\n",
    "    #     # throw error\n",
    "    #     print(\"No pre-collected data found. Please run the data collection script first.\")\n",
    "    #     exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 43\n"
     ]
    }
   ],
   "source": [
    "historic_data = pd.read_csv(\"Datasets/eth_historical_data_simple.csv\", index_col=0)\n",
    "\n",
    "historic_data.head()\n",
    "\n",
    "# print num of columns\n",
    "print('Number of columns: ' + str(len(historic_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_INVESTMENT = 1000\n",
    "NUM_EPISODES = 1000\n",
    "START_DATE = \"2017-01-01-00-00\"\n",
    "END_DATE = \"2020-01-01-00-00\"\n",
    "TESTING_SPLIT = \"2022-01-01-00:00:00\" # formatting is different here because of how historic_crypto indexes dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-collected data loaded successfully.\n",
      "NaN values dropped.\n",
      "Data split.\n",
      "Train data saved.\n",
      "Test data saved.\n",
      "\n",
      "===== Episode 1 / 1 =====\n",
      "Time step 0 / 956   |  Eth Held: 0.0  |  Cash Held: 1000.0  |  Portfolio Value: 0.0  |   MSE Loss: 0.0\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Time step 100 / 956   |  Eth Held: 1.5656261  |  Cash Held: 274.03  |  Portfolio Value: -437.514  |   MSE Loss: 7624.953\n",
      "Time step 200 / 956   |  Eth Held: 0.834988  |  Cash Held: 126.84  |  Portfolio Value: -715.141  |   MSE Loss: 3107.624\n",
      "Time step 300 / 956   |  Eth Held: 0.953315  |  Cash Held: 0.0  |  Portfolio Value: -806.526  |   MSE Loss: 129.938\n",
      "Time step 400 / 956   |  Eth Held: 0.4770232  |  Cash Held: 19.28  |  Portfolio Value: -791.898  |   MSE Loss: 42.383\n",
      "Time step 500 / 956   |  Eth Held: 0.4779359  |  Cash Held: 22.17  |  Portfolio Value: -683.978  |   MSE Loss: 38.701\n",
      "Time step 600 / 956   |  Eth Held: 0.3766417  |  Cash Held: 96.05  |  Portfolio Value: -226.312  |   MSE Loss: 122.941\n",
      "Time step 700 / 956   |  Eth Held: 0.1782089  |  Cash Held: 107.76  |  Portfolio Value: -537.654  |   MSE Loss: 34492500.0\n",
      "Time step 800 / 956   |  Eth Held: 0.0  |  Cash Held: 39.44  |  Portfolio Value: -960.557  |   MSE Loss: 61348208640.0\n",
      "Time step 0 / 240   |   Eth Held: 0.3913837  |  Cash Held: -0.0  |  Portfolio Value: -964.448\n",
      "Time step 1 / 240   |   Eth Held: 0.0  |  Cash Held: 928.71  |  Portfolio Value: -964.448\n",
      "Time step 2 / 240   |   Eth Held: 0.3604186  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 3 / 240   |   Eth Held: 0.0  |  Cash Held: 934.67  |  Portfolio Value: -964.448\n",
      "Time step 4 / 240   |   Eth Held: 0.3583796  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 5 / 240   |   Eth Held: 0.0  |  Cash Held: 871.43  |  Portfolio Value: -964.448\n",
      "Time step 6 / 240   |   Eth Held: 0.3384519  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 7 / 240   |   Eth Held: 0.0  |  Cash Held: 809.91  |  Portfolio Value: -964.448\n",
      "Time step 8 / 240   |   Eth Held: 0.3126243  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 9 / 240   |   Eth Held: 0.0  |  Cash Held: 778.17  |  Portfolio Value: -964.448\n",
      "Time step 10 / 240   |   Eth Held: 0.2807181  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 11 / 240   |   Eth Held: 0.0  |  Cash Held: 750.67  |  Portfolio Value: -964.448\n",
      "Time step 12 / 240   |   Eth Held: 0.2548673  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 13 / 240   |   Eth Held: 0.0  |  Cash Held: 713.36  |  Portfolio Value: -964.448\n",
      "Time step 14 / 240   |   Eth Held: 0.2493863  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 15 / 240   |   Eth Held: 0.0  |  Cash Held: 686.58  |  Portfolio Value: -964.448\n",
      "Time step 16 / 240   |   Eth Held: 0.2309282  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 17 / 240   |   Eth Held: 0.0  |  Cash Held: 664.96  |  Portfolio Value: -964.448\n",
      "Time step 18 / 240   |   Eth Held: 0.2139471  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 19 / 240   |   Eth Held: 0.0  |  Cash Held: 631.43  |  Portfolio Value: -964.448\n",
      "Time step 20 / 240   |   Eth Held: 0.2008891  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 21 / 240   |   Eth Held: 0.0  |  Cash Held: 628.18  |  Portfolio Value: -964.448\n",
      "Time step 22 / 240   |   Eth Held: 0.1882675  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 23 / 240   |   Eth Held: 0.0  |  Cash Held: 608.46  |  Portfolio Value: -964.448\n",
      "Time step 24 / 240   |   Eth Held: 0.1797433  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 25 / 240   |   Eth Held: 0.0  |  Cash Held: 560.36  |  Portfolio Value: -964.448\n",
      "Time step 26 / 240   |   Eth Held: 0.1624445  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 27 / 240   |   Eth Held: 0.0  |  Cash Held: 531.65  |  Portfolio Value: -964.448\n",
      "Time step 28 / 240   |   Eth Held: 0.1509153  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 29 / 240   |   Eth Held: 0.0  |  Cash Held: 504.84  |  Portfolio Value: -964.448\n",
      "Time step 30 / 240   |   Eth Held: 0.1479687  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 31 / 240   |   Eth Held: 0.0  |  Cash Held: 445.85  |  Portfolio Value: -964.448\n",
      "Time step 32 / 240   |   Eth Held: 0.1378929  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 33 / 240   |   Eth Held: 0.0  |  Cash Held: 418.16  |  Portfolio Value: -964.448\n",
      "Time step 34 / 240   |   Eth Held: 0.1281934  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 35 / 240   |   Eth Held: 0.0  |  Cash Held: 391.15  |  Portfolio Value: -964.448\n",
      "Time step 36 / 240   |   Eth Held: 0.1312131  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 37 / 240   |   Eth Held: 0.0  |  Cash Held: 377.74  |  Portfolio Value: -964.448\n",
      "Time step 38 / 240   |   Eth Held: 0.121136  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 39 / 240   |   Eth Held: 0.0  |  Cash Held: 347.53  |  Portfolio Value: -964.448\n",
      "Time step 40 / 240   |   Eth Held: 0.1142843  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 41 / 240   |   Eth Held: 0.0  |  Cash Held: 332.48  |  Portfolio Value: -964.448\n",
      "Time step 42 / 240   |   Eth Held: 0.1110693  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 43 / 240   |   Eth Held: 0.0  |  Cash Held: 322.63  |  Portfolio Value: -964.448\n",
      "Time step 44 / 240   |   Eth Held: 0.1039352  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 45 / 240   |   Eth Held: 0.0  |  Cash Held: 303.89  |  Portfolio Value: -964.448\n",
      "Time step 46 / 240   |   Eth Held: 0.1017218  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 47 / 240   |   Eth Held: 0.0  |  Cash Held: 286.51  |  Portfolio Value: -964.448\n",
      "Time step 48 / 240   |   Eth Held: 0.0975146  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 49 / 240   |   Eth Held: 0.0  |  Cash Held: 270.76  |  Portfolio Value: -964.448\n",
      "Time step 50 / 240   |   Eth Held: 0.0899711  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 51 / 240   |   Eth Held: 0.0  |  Cash Held: 240.03  |  Portfolio Value: -964.448\n",
      "Time step 52 / 240   |   Eth Held: 0.083087  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 53 / 240   |   Eth Held: 0.0  |  Cash Held: 231.82  |  Portfolio Value: -964.448\n",
      "Time step 54 / 240   |   Eth Held: 0.0823343  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 55 / 240   |   Eth Held: 0.0  |  Cash Held: 213.55  |  Portfolio Value: -964.448\n",
      "Time step 56 / 240   |   Eth Held: 0.0755187  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 57 / 240   |   Eth Held: 0.0  |  Cash Held: 205.0  |  Portfolio Value: -964.448\n",
      "Time step 58 / 240   |   Eth Held: 0.0736484  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 59 / 240   |   Eth Held: 0.0  |  Cash Held: 205.75  |  Portfolio Value: -964.448\n",
      "Time step 60 / 240   |   Eth Held: 0.0748378  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 61 / 240   |   Eth Held: 0.0  |  Cash Held: 191.6  |  Portfolio Value: -964.448\n",
      "Time step 62 / 240   |   Eth Held: 0.0726841  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 63 / 240   |   Eth Held: 0.0  |  Cash Held: 173.83  |  Portfolio Value: -964.448\n",
      "Time step 64 / 240   |   Eth Held: 0.0774152  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 65 / 240   |   Eth Held: 0.0  |  Cash Held: 172.35  |  Portfolio Value: -964.448\n",
      "Time step 66 / 240   |   Eth Held: 0.0831772  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 67 / 240   |   Eth Held: 0.0  |  Cash Held: 155.01  |  Portfolio Value: -964.448\n",
      "Time step 68 / 240   |   Eth Held: 0.0769504  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 69 / 240   |   Eth Held: 0.0  |  Cash Held: 150.32  |  Portfolio Value: -964.448\n",
      "Time step 70 / 240   |   Eth Held: 0.070056  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 71 / 240   |   Eth Held: 0.0  |  Cash Held: 134.62  |  Portfolio Value: -964.448\n",
      "Time step 72 / 240   |   Eth Held: 0.0643983  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 73 / 240   |   Eth Held: 0.0  |  Cash Held: 117.26  |  Portfolio Value: -964.448\n",
      "Time step 74 / 240   |   Eth Held: 0.0580964  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 75 / 240   |   Eth Held: 0.0  |  Cash Held: 108.25  |  Portfolio Value: -964.448\n",
      "Time step 76 / 240   |   Eth Held: 0.0548225  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 77 / 240   |   Eth Held: 0.0  |  Cash Held: 106.41  |  Portfolio Value: -964.448\n",
      "Time step 78 / 240   |   Eth Held: 0.053956  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 79 / 240   |   Eth Held: 0.0  |  Cash Held: 101.44  |  Portfolio Value: -964.448\n",
      "Time step 80 / 240   |   Eth Held: 0.0521584  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 81 / 240   |   Eth Held: 0.0  |  Cash Held: 89.38  |  Portfolio Value: -964.448\n",
      "Time step 82 / 240   |   Eth Held: 0.0518196  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 83 / 240   |   Eth Held: 0.0  |  Cash Held: 86.54  |  Portfolio Value: -964.448\n",
      "Time step 84 / 240   |   Eth Held: 0.0477591  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 85 / 240   |   Eth Held: 0.0  |  Cash Held: 90.58  |  Portfolio Value: -964.448\n",
      "Time step 86 / 240   |   Eth Held: 0.0466352  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 87 / 240   |   Eth Held: 0.0  |  Cash Held: 80.79  |  Portfolio Value: -964.448\n",
      "Time step 88 / 240   |   Eth Held: 0.0440479  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 89 / 240   |   Eth Held: 0.0  |  Cash Held: 74.28  |  Portfolio Value: -964.448\n",
      "Time step 90 / 240   |   Eth Held: 0.0412293  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 91 / 240   |   Eth Held: 0.0  |  Cash Held: 70.71  |  Portfolio Value: -964.448\n",
      "Time step 92 / 240   |   Eth Held: 0.0380284  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 93 / 240   |   Eth Held: 0.0  |  Cash Held: 65.54  |  Portfolio Value: -964.448\n",
      "Time step 94 / 240   |   Eth Held: 0.0365395  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 95 / 240   |   Eth Held: 0.0  |  Cash Held: 62.13  |  Portfolio Value: -964.448\n",
      "Time step 96 / 240   |   Eth Held: 0.037314  |  Cash Held: -0.0  |  Portfolio Value: -964.448\n",
      "Time step 97 / 240   |   Eth Held: 0.0  |  Cash Held: 54.22  |  Portfolio Value: -964.448\n",
      "Time step 98 / 240   |   Eth Held: 0.0375196  |  Cash Held: -0.0  |  Portfolio Value: -964.448\n",
      "Time step 99 / 240   |   Eth Held: 0.0  |  Cash Held: 42.94  |  Portfolio Value: -964.448\n",
      "Time step 100 / 240   |   Eth Held: 0.0354353  |  Cash Held: -0.0  |  Portfolio Value: -964.448\n",
      "Time step 101 / 240   |   Eth Held: 0.0  |  Cash Held: 41.51  |  Portfolio Value: -964.448\n",
      "Time step 102 / 240   |   Eth Held: 0.0388807  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 103 / 240   |   Eth Held: 0.0  |  Cash Held: 40.13  |  Portfolio Value: -964.448\n",
      "Time step 104 / 240   |   Eth Held: 0.0403894  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 105 / 240   |   Eth Held: 0.0  |  Cash Held: 43.27  |  Portfolio Value: -964.448\n",
      "Time step 106 / 240   |   Eth Held: 0.0383704  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 107 / 240   |   Eth Held: 0.0  |  Cash Held: 41.0  |  Portfolio Value: -964.448\n",
      "Time step 108 / 240   |   Eth Held: 0.0389967  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 109 / 240   |   Eth Held: 0.0  |  Cash Held: 42.36  |  Portfolio Value: -964.448\n",
      "Time step 110 / 240   |   Eth Held: 0.0345267  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 111 / 240   |   Eth Held: 0.0  |  Cash Held: 40.79  |  Portfolio Value: -964.448\n",
      "Time step 112 / 240   |   Eth Held: 0.0339927  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 113 / 240   |   Eth Held: 0.0  |  Cash Held: 38.55  |  Portfolio Value: -964.448\n",
      "Time step 114 / 240   |   Eth Held: 0.0336784  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 115 / 240   |   Eth Held: 0.0  |  Cash Held: 35.16  |  Portfolio Value: -964.448\n",
      "Time step 116 / 240   |   Eth Held: 0.0329431  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 117 / 240   |   Eth Held: 0.0  |  Cash Held: 33.17  |  Portfolio Value: -964.448\n",
      "Time step 118 / 240   |   Eth Held: 0.031098  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 119 / 240   |   Eth Held: 0.0  |  Cash Held: 31.72  |  Portfolio Value: -964.448\n",
      "Time step 120 / 240   |   Eth Held: 0.0275593  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 121 / 240   |   Eth Held: 0.0  |  Cash Held: 29.7  |  Portfolio Value: -964.448\n",
      "Time step 122 / 240   |   Eth Held: 0.0250248  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 123 / 240   |   Eth Held: 0.0  |  Cash Held: 29.42  |  Portfolio Value: -964.448\n",
      "Time step 124 / 240   |   Eth Held: 0.024067  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 125 / 240   |   Eth Held: 0.0  |  Cash Held: 27.82  |  Portfolio Value: -964.448\n",
      "Time step 126 / 240   |   Eth Held: 0.0238142  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 127 / 240   |   Eth Held: 0.0  |  Cash Held: 24.82  |  Portfolio Value: -964.448\n",
      "Time step 128 / 240   |   Eth Held: 0.0239101  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 129 / 240   |   Eth Held: 0.0  |  Cash Held: 25.29  |  Portfolio Value: -964.448\n",
      "Time step 130 / 240   |   Eth Held: 0.0212289  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 131 / 240   |   Eth Held: 0.0  |  Cash Held: 24.87  |  Portfolio Value: -964.448\n",
      "Time step 132 / 240   |   Eth Held: 0.0183857  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 133 / 240   |   Eth Held: 0.0  |  Cash Held: 23.38  |  Portfolio Value: -964.448\n",
      "Time step 134 / 240   |   Eth Held: 0.0148102  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 135 / 240   |   Eth Held: 0.0  |  Cash Held: 21.71  |  Portfolio Value: -964.448\n",
      "Time step 136 / 240   |   Eth Held: 0.0142805  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 137 / 240   |   Eth Held: 0.0  |  Cash Held: 21.39  |  Portfolio Value: -964.448\n",
      "Time step 138 / 240   |   Eth Held: 0.0139137  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 139 / 240   |   Eth Held: 0.0  |  Cash Held: 20.48  |  Portfolio Value: -964.448\n",
      "Time step 140 / 240   |   Eth Held: 0.0128033  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 141 / 240   |   Eth Held: 0.0  |  Cash Held: 17.58  |  Portfolio Value: -964.448\n",
      "Time step 142 / 240   |   Eth Held: 0.0121933  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 143 / 240   |   Eth Held: 0.0  |  Cash Held: 18.95  |  Portfolio Value: -964.448\n",
      "Time step 144 / 240   |   Eth Held: 0.0109846  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 145 / 240   |   Eth Held: 0.0  |  Cash Held: 18.03  |  Portfolio Value: -964.448\n",
      "Time step 146 / 240   |   Eth Held: 0.0106288  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 147 / 240   |   Eth Held: 0.0  |  Cash Held: 16.98  |  Portfolio Value: -964.448\n",
      "Time step 148 / 240   |   Eth Held: 0.0103834  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "Time step 149 / 240   |   Eth Held: 0.0  |  Cash Held: 16.11  |  Portfolio Value: -964.448\n",
      "Time step 150 / 240   |   Eth Held: 0.00995  |  Cash Held: 0.0  |  Portfolio Value: -964.448\n",
      "WARNING:tensorflow:From c:\\Users\\ericb\\anaconda3\\envs\\machinelearning_20220719\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: Output\\online_model\\assets\n",
      "INFO:tensorflow:Assets written to: Output\\target_model\\assets\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # train_df, test_df = get_crypto_data(START_DATE, END_DATE, TESTING_SPLIT)\n",
    "    train_df, test_df, train_close, test_close = get_precollected_data(split_ratio=0.8)\n",
    "    trading_agent = Agent(train_df.shape, NUM_EPISODES, window_size=48, gamma=0.95, update_interval=96, lr=0.01, min_epsilon=0.02)\n",
    "    episode_mem = [{\"Actions\": [], \"Eth Held\": [], \"Cash Held\": [], \"Portfolio Value\": [], \"Reward\": [], \"Done\": [], \"Epsilon\": [], \"MSE Loss\": []} for i in range(NUM_EPISODES)]\n",
    "    t0 = time()\n",
    "\n",
    "    ######################## Training ########################\n",
    "    for s in range(NUM_EPISODES):\n",
    "        print(f\"\\n===== Episode {s + 1} / {NUM_EPISODES} =====\")\n",
    "        state = trading_agent.get_state(0, train_df)\n",
    "        cash_balance = INITIAL_INVESTMENT\n",
    "        portfolio_value_usd = INITIAL_INVESTMENT\n",
    "        # Reset the agent's portfolio at the beginning of each episode\n",
    "        trading_agent.portfolio = [0 , INITIAL_INVESTMENT, portfolio_value_usd]\n",
    "\n",
    "        done = False\n",
    "        for t in range(len(train_df) - 1):\n",
    "            if done:\n",
    "                break\n",
    "            action = trading_agent.get_action(state, cash_balance)\n",
    "            next_state = trading_agent.get_state(t + 1, train_df)\n",
    "            reward = trading_agent.trade(t, action, train_df, train_close, INITIAL_INVESTMENT, trading_fee_rate = 0.05)\n",
    "            eth_held, cash_held, new_portfolio_value = trading_agent.portfolio\n",
    "            cash_balance = cash_held  # update cash balance\n",
    "            portfolio_value_usd = new_portfolio_value  # update portfolio value\n",
    "            \n",
    "            if t != 0:  # if not the first trade\n",
    "                done = cash_balance <= 1 and eth_held <= 0.01\n",
    "            trading_agent.memory.add_exp(state, action, reward, next_state, done)\n",
    "            loss = trading_agent.train()\n",
    "            if not loss:\n",
    "                loss = 0\n",
    "            state = next_state\n",
    "            \n",
    "            episode_mem[s][\"Actions\"].append(int(action))\n",
    "            episode_mem[s][\"Eth Held\"].append(float(eth_held))\n",
    "            episode_mem[s][\"Cash Held\"].append(round(float(cash_balance), 2))\n",
    "            episode_mem[s][\"Portfolio Value\"].append(float(portfolio_value_usd))\n",
    "            episode_mem[s][\"Reward\"].append(float(reward))\n",
    "            episode_mem[s][\"Done\"].append(bool(done))\n",
    "            episode_mem[s][\"Epsilon\"].append(trading_agent.epsilon)\n",
    "            episode_mem[s][\"MSE Loss\"].append(float(loss))\n",
    "\n",
    "            if t % 100 == 0:\n",
    "                print(f\"Time step {t} / {len(train_df)}   |  Eth Held: {round(episode_mem[s]['Eth Held'][t], 7)}  |  Cash Held: {round(episode_mem[s]['Cash Held'][t], 2)}  |  Portfolio Value: {round(episode_mem[s]['Portfolio Value'][t], 3)}  |   MSE Loss: {round(episode_mem[s]['MSE Loss'][t], 3)}\")\n",
    "\n",
    "        with open('Output/training_scores.out', 'a') as f:\n",
    "            f.write(f\"EPISODE {s} (runtime: {time() - t0})   | Portfolio Value is {round(episode_mem[s]['Portfolio Value'][-1], 3)} Epsilon is {round(trading_agent.epsilon, 3)}   |   MSE Loss is {round(episode_mem[s]['MSE Loss'][-1], 3)}\\n\")\n",
    "\n",
    "    with open('Output/episode_mem.json', 'w') as f:\n",
    "        json.dump(episode_mem, f)\n",
    "\n",
    "    ######################## Testing ########################\n",
    "    t0 = time()\n",
    "    testing_mem = {\"Actions\": [], \"Eth Held\": [], \"Cash Held\": [], \"Portfolio Value\": [], \"Reward\": [], \"Done\": []}\n",
    "    trading_agent.epsilon = 0\n",
    "    state = trading_agent.get_state(0, test_df)\n",
    "    cash_balance = INITIAL_INVESTMENT\n",
    "    portfolio_value_usd = INITIAL_INVESTMENT\n",
    "    # Reset the agent's portfolio at the beginning of each episode\n",
    "    trading_agent.portfolio = [0 , INITIAL_INVESTMENT, portfolio_value_usd]\n",
    "    # Reset the agent's portfolio at the beginning of each episode\n",
    "\n",
    "    done = False\n",
    "    for t in range(len(test_df) - 1):\n",
    "        if done:\n",
    "            break\n",
    "        action = trading_agent.get_action(state, cash_balance)\n",
    "        next_state = trading_agent.get_state(t + 1, test_df)\n",
    "        reward = trading_agent.trade(t, action, test_df, test_close, INITIAL_INVESTMENT, trading_fee_rate = 0.05)\n",
    "        eth_held, cash_held, portfolio_value_usd  = trading_agent.portfolio\n",
    "        cash_balance = cash_held  # update cash balance\n",
    "        portfolio_value_usd = new_portfolio_value  # update portfolio value\n",
    "        if t != 0:  # if not the first trade\n",
    "                done = cash_balance <= 1 and eth_held <= 0.01\n",
    "        state = next_state\n",
    "\n",
    "        testing_mem[\"Actions\"].append(int(action))\n",
    "        testing_mem[\"Eth Held\"].append(float(eth_held))\n",
    "        testing_mem[\"Cash Held\"].append(round(float(cash_balance),2))\n",
    "        testing_mem[\"Portfolio Value\"].append(float(portfolio_value_usd))\n",
    "        testing_mem[\"Reward\"].append(float(reward))\n",
    "        testing_mem[\"Done\"].append(bool(done))\n",
    "\n",
    "        if t % 1 == 0:\n",
    "            print(f\"Time step {t} / {len(test_df)}   |   Eth Held: {round(testing_mem['Eth Held'][t], 7)}  |  Cash Held: {round(testing_mem['Cash Held'][t], 2)}  |  Portfolio Value: {round(testing_mem['Portfolio Value'][t], 3)}\")\n",
    "\n",
    "    with open('Output/testing_scores.out', 'a') as f:\n",
    "        f.write(f\"TESTING (runtime: {time() - t0})   |  Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\\n\")\n",
    "\n",
    "    with open('Output/testing_mem.json', 'w') as f:\n",
    "        json.dump(testing_mem, f)\n",
    "\n",
    "    trading_agent.save_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-collected data loaded successfully.\n",
      "NaN values dropped.\n",
      "Data split.\n",
      "Train data saved.\n",
      "Test data saved.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14284\\282523319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mtesting_mem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mplot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_mem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Training Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df, test_df, train_close, test_close = get_precollected_data(split_ratio=0.8)\n",
    "\n",
    "def plot_data(data, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_df[\"Close\"], label=\"Ethereum Price\")\n",
    "    plt.plot(data[\"Portfolio Value\"], label=\"Portfolio Value\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "# visualize training data\n",
    "with open('Output/episode_mem.json', 'r') as f:\n",
    "    episode_mem = json.load(f)\n",
    "\n",
    "with open(\"Output/testing_mem.json\", \"r\") as f:\n",
    "    testing_mem = json.load(f)\n",
    "\n",
    "plot_data(episode_mem[29], \"Training Data\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot((testing_mem[\"Portfolio Value\"]))\n",
    "plt.plot(test_df['Close'])\n",
    "plt.legend([\"Ethereum Price\", \"Average Portfolio Value\"])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()\n",
    "# plot ethereum price history\n",
    "# set plot width and height\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(historic_data[\"Close\"])\n",
    "plt.title(\"Ethereum Price History\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()\n",
    "\n",
    "# # plot average portfolio value\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Portfolio Value\"])\n",
    "# plt.legend([\"Ethereum Price\", \"Average Portfolio Value\"])\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"Price\")\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Portfolio Value\"])\n",
    "# plt.title(\"Portfolio Value\")\n",
    "# plt.show()\n",
    "\n",
    "for i in range(NUM_EPISODES):\n",
    "    plt.plot(episode_mem[i][\"Realized Profit\"])\n",
    "plt.title(\"Realized Profit\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(episode_mem[0][\"Realized Profit\"])\n",
    "plt.title(\"Realized Profit\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(episode_mem[29][\"Realized Profit\"])\n",
    "plt.title(\"Realized Profit\")\n",
    "plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Reward\"])\n",
    "# plt.title(\"Reward\")\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Epsilon\"])\n",
    "# plt.title(\"Epsilon\")\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"MSE Loss\"])\n",
    "# plt.title(\"MSE Loss\")\n",
    "# plt.show()\n",
    "\n",
    "# # visualize testing data\n",
    "# with open('Output/testing_mem.json', 'r') as f:\n",
    "#     testing_mem = json.load(f)\n",
    "\n",
    "# # plot portfolio value vs ethereum price\n",
    "# plt.plot(testing_mem[\"Portfolio Value\"])\n",
    "# plt.title(\"Portfolio Value\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(testing_mem[\"Realized Profit\"])\n",
    "# plt.title(\"Realized Profit\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(testing_mem[\"Reward\"])\n",
    "# plt.title(\"Reward\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning_20220719",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23361560846584cf1ea895074b40a7ea878b24065d1c129dab67edcdc8abadf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
