{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from time import time\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Historic_Crypto import HistoricalData\n",
    "from Historic_Crypto import Cryptocurrencies\n",
    "from Historic_Crypto import LiveCryptoData\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version \" + torch.__version__)\n",
    "print(\"Num GPUs Available: \", torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQN(torch.nn.Module):\n",
    "    def __init__(self, input_features, window_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_features * window_size\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(negative_slope=0.1)\n",
    "        self.d1 = torch.nn.Linear(self.input_size, 256)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(256)\n",
    "        self.d2 = torch.nn.Linear(256, 512)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(512)\n",
    "        self.drop1 = torch.nn.Dropout(0.3)\n",
    "        self.d3 = torch.nn.Linear(512, 512)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(512)\n",
    "        self.drop2 = torch.nn.Dropout(0.3)\n",
    "        self.d4 = torch.nn.Linear(512, 256)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(256)\n",
    "        self.drop3 = torch.nn.Dropout(0.3)\n",
    "        self.dv1 = torch.nn.Linear(256, 128)  # value hidden layer\n",
    "        self.da1 = torch.nn.Linear(256, 128)  # actions hidden layer\n",
    "        self.dv2 = torch.nn.Linear(128, 1)  # value output\n",
    "        self.da2 = torch.nn.Linear(128, 9)  # actions output\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        input_data = input_data.reshape(input_data.size(0), -1)  # Flatten the input tensor\n",
    "        x = self.leaky_relu(self.d1(input_data))\n",
    "        x = self.bn1(x)\n",
    "        x = x.view(x.size(0), -1)  # equivalent to Flatten()\n",
    "        x = self.leaky_relu(self.d2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.leaky_relu(self.d3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.leaky_relu(self.d4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.drop3(x)\n",
    "        v = self.leaky_relu(self.dv1(x))\n",
    "        a = self.leaky_relu(self.da1(x))\n",
    "        v = self.dv2(v)\n",
    "        a = self.da2(a)\n",
    "        Q = v + (a - torch.mean(a, dim=1, keepdim=True))\n",
    "        return Q\n",
    "\n",
    "    def advantage(self, state):\n",
    "        x = self.leaky_relu(self.d1(state))\n",
    "        x = self.bn1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.leaky_relu(self.d2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.leaky_relu(self.d3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.leaky_relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpReplay():\n",
    "    def __init__(self, num_features, window_size, device, buffer_size=1000000):\n",
    "        self.num_features = num_features\n",
    "        self.device = device\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_mem = np.zeros((self.buffer_size, self.num_features, window_size), dtype=np.float32)\n",
    "        self.action_mem = np.ones((self.buffer_size), dtype=np.int32)\n",
    "        self.reward_mem = np.zeros((self.buffer_size), dtype=np.compat.long)\n",
    "        self.next_state_mem = np.zeros((self.buffer_size, self.num_features, window_size), dtype=np.float32)\n",
    "        self.done_mem = np.zeros((self.buffer_size), dtype=bool)\n",
    "        self.counter = 0\n",
    "\n",
    "    def add_exp(self, state, action, reward, next_state, done):\n",
    "        pointer = self.counter % self.buffer_size\n",
    "        self.state_mem[pointer] = state\n",
    "        self.action_mem[pointer] = action\n",
    "        self.reward_mem[pointer] = reward\n",
    "        self.next_state_mem[pointer] = next_state\n",
    "        self.done_mem[pointer] = 1 - int(done)\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample_exp(self, batch_size=64):\n",
    "        max_mem = min(self.counter, self.buffer_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        states = torch.tensor(self.state_mem[batch], dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(self.action_mem[batch], dtype=torch.int64).to(self.device)\n",
    "        rewards = torch.tensor(self.reward_mem[batch], dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(self.next_state_mem[batch], dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(self.done_mem[batch], dtype=torch.bool).to(self.device)\n",
    "        return states, actions, rewards, next_states, dones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, data_shape, num_episodes, window_size=48, gamma=0.99, update_interval=96, lr=0.01, min_epsilon=0.02):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.window_size = window_size\n",
    "        self.data_shape = data_shape\n",
    "        self.portfolio = [0, 0, 0]  # [total eth, cash_held, total_portfolio_value (eth value + cash held - initial investment)]\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = 1.0\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.update_interval = update_interval\n",
    "        self.trainstep = 0\n",
    "        self.memory = ExpReplay(self.window_size, data_shape[1], self.device)        \n",
    "        self.batch_size = 64\n",
    "        self.online_net = DDDQN(self.data_shape[1], window_size).to(self.device)\n",
    "        self.target_net = DDDQN(self.data_shape[1], window_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "        initial_learning_rate = lr\n",
    "        decay_steps = self.num_episodes * self.data_shape[0] // 10  # You can adjust the divisor to control the decay rate\n",
    "        decay_rate = 0.9  # You can adjust this value to control the decay rate\n",
    "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=initial_learning_rate)\n",
    "        self.scheduler = ExponentialLR(self.optimizer, gamma=decay_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, state, cash_balance):\n",
    "        state_numpy = state.values\n",
    "        state_tensor = torch.FloatTensor(state_numpy).to(self.device).unsqueeze(0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            if self.portfolio[0] > 0.01:\n",
    "                return np.random.choice([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "            elif cash_balance > 0:\n",
    "                return np.random.choice([4, 5, 6, 7, 8])\n",
    "            else:\n",
    "                action = 4  # hold\n",
    "                return action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                self.online_net.eval()  # Set the model to evaluation mode\n",
    "                actions = self.online_net(state_tensor)\n",
    "                self.online_net.train()  # Set the model back to training mode\n",
    "                if self.portfolio[0] > 0.01:\n",
    "                    action = torch.argmax(actions).item()\n",
    "                elif cash_balance > 0:\n",
    "                    action = torch.argmax(actions[0, 4:]) + 4\n",
    "                else:\n",
    "                    action = 4  # hold action\n",
    "            return action\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            b = self.min_epsilon**(1/(self.num_episodes*self.data_shape[0]))\n",
    "            self.epsilon = b**self.trainstep\n",
    "\n",
    "    def train(self):\n",
    "        if self.memory.counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        if self.trainstep % self.update_interval == 0:\n",
    "            self.update_target()\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size)\n",
    "\n",
    "\n",
    "        # Move tensors to the device and set the correct data type\n",
    "        states = states.to(self.device).float()\n",
    "        actions = actions.to(self.device).long()\n",
    "        rewards = rewards.to(self.device).float()\n",
    "        next_states = next_states.to(self.device).float()\n",
    "        dones = dones.to(self.device).float()\n",
    "        \n",
    "\n",
    "        # print(\"states shape: \", states.shape)\n",
    "        # print(\"actions shape: \", actions.shape)\n",
    "        # print(\"rewards shape: \", rewards.shape)\n",
    "        # print(\"next_states shape: \", next_states.shape)\n",
    "        # print(\"dones shape: \", dones.shape)\n",
    "\n",
    "        q_next_state_online_net = self.online_net(next_states)\n",
    "        q_next_state_target_net = self.target_net(next_states)\n",
    "\n",
    "        max_action = torch.argmax(q_next_state_online_net, dim=1).to(self.device)\n",
    "\n",
    "        batch_index = torch.arange(self.batch_size, dtype=torch.int64).to(self.device)\n",
    "\n",
    "        q_predicted = self.online_net(states)\n",
    "        q_target = q_predicted.clone().detach()\n",
    "\n",
    "        q_target[batch_index, actions] = rewards + self.gamma * q_next_state_target_net[batch_index, max_action] * dones\n",
    "\n",
    "        loss = self.criterion(q_predicted, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_epsilon()\n",
    "        self.trainstep += 1\n",
    "        return loss.item()\n",
    "\n",
    "    def save_model(self):\n",
    "        import os\n",
    "\n",
    "        output_directory = \"Output\"\n",
    "        online_model_directory = os.path.join(output_directory, \"online_model\")\n",
    "        target_model_directory = os.path.join(output_directory, \"target_model\")\n",
    "\n",
    "        os.makedirs(online_model_directory, exist_ok=True)\n",
    "        os.makedirs(target_model_directory, exist_ok=True)\n",
    "\n",
    "        torch.save(self.online_net.state_dict(), os.path.join(online_model_directory, 'model.pt'))\n",
    "        torch.save(self.target_net.state_dict(), os.path.join(target_model_directory, 'model.pt'))\n",
    "\n",
    "    def calculate_reward(self, t, eth_df, eth_close_price_unscaled, amount_to_sell, initial_investment, trading_fee_rate):\n",
    "        ether_held, cash_held, previous_portfolio_value = self.portfolio\n",
    "        \n",
    "        unscaled_close_price = eth_close_price_unscaled[\"Close\"].iloc[t]\n",
    "        value_of_eth_sold = unscaled_close_price * amount_to_sell\n",
    "        trading_fee = value_of_eth_sold * trading_fee_rate\n",
    "        cash_received = value_of_eth_sold - trading_fee\n",
    "        new_cash_held = cash_held + cash_received\n",
    "        new_ether_held = ether_held - amount_to_sell\n",
    "        \n",
    "        # Calculate portfolio value\n",
    "        new_portfolio_value = new_cash_held + (new_ether_held * unscaled_close_price) - initial_investment\n",
    "        \n",
    "        # Calculate the reward based on the change in portfolio value\n",
    "        reward = new_portfolio_value - previous_portfolio_value\n",
    "\n",
    "        # Update portfolio\n",
    "        self.portfolio = [new_ether_held, new_cash_held, new_portfolio_value]\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def trade(self, t, action, eth_df, eth_df_unscaled, initial_investment, trading_fee_rate):\n",
    "        reward = 0\n",
    "        eth_held, cash_balance, previous_portfolio_value = self.portfolio\n",
    "        sell_percentages = [0.25, 0.5, 0.75, 1.0]\n",
    "        buy_percentages = [0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "        if action >= 0 and action <= 3 and eth_held > 0.01:\n",
    "            # print(\"Selling: \" + str(sell_percentages[action]) + \"% of portfolio at eth price \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "            # print(\"Current cash_balance: \" + str(round(cash_balance, 2)))\n",
    "            sell_percentage = sell_percentages[action]\n",
    "            amount_to_sell = eth_held * sell_percentage\n",
    "\n",
    "            while amount_to_sell > 0 and self.portfolio[0] > 0.01:\n",
    "                scaled_item_cost = eth_df[\"Close\"].iloc[t]\n",
    "                unscaled_item_cost = eth_df_unscaled[\"Close\"].iloc[t]\n",
    "                # print(\"Eth amount: \" + str(round(eth_held, 7)))\n",
    "                # print(\"Current ask price: \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "                # print('----------------')\n",
    "\n",
    "                reward = self.calculate_reward(t, eth_df, eth_df_unscaled, amount_to_sell, initial_investment, trading_fee_rate)\n",
    "\n",
    "                eth_held, cash_balance, current_portfolio_value = self.portfolio # Update portfolio after selling\n",
    "\n",
    "                # print(\"Amount Eth sold: \" + str(amount_to_sell))\n",
    "                # print(\"Selling price: \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "                # print(\"Trading fee: \" + str(trading_fee_rate * unscaled_item_cost))\n",
    "                # print(\"New Eth amount: \" + str(round(eth_held, 7)))\n",
    "                # print(\"New cash_balance: \" + str(round(cash_balance, 2)))\n",
    "                amount_to_sell = 0\n",
    "        elif action == 4:\n",
    "            # print(\"Hold: Price is \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "            self.portfolio[2] = cash_balance + (eth_held * eth_df_unscaled[\"Close\"].iloc[t]) - initial_investment\n",
    "            reward = -0.01\n",
    "        elif action >= 5 and cash_balance >= 0:\n",
    "            # print(\"Buy Ether with: \" + str(buy_percentages[action - 5]) + \"% of cash_balance at Eth price \" + str(eth_df_unscaled[\"Close\"].iloc[t]))\n",
    "            buy_percentage = buy_percentages[action - 5]\n",
    "            eth_to_buy = (cash_balance * buy_percentage) / eth_df_unscaled[\"Close\"].iloc[t]\n",
    "            # print(\"Current cash_balance: \" + str(round(cash_balance, 2)))\n",
    "            # print(\"Current Eth amount: \" + str(round(eth_held, 7)))\n",
    "            # print(\"Eth purchased: \" + str(eth_to_buy))\n",
    "            self.portfolio[0] += eth_to_buy\n",
    "            self.portfolio[1] -= eth_df_unscaled[\"Close\"].iloc[t] * eth_to_buy\n",
    "            self.portfolio[2] = self.portfolio[1] + (self.portfolio[0] * eth_df_unscaled[\"Close\"].iloc[t]) - initial_investment\n",
    "            # print(\"New cash_balance: \" + str(round(self.portfolio[1], 2)))\n",
    "            # add some reward for buying if a buy flag is set?\n",
    "        return reward\n",
    "    \n",
    "    def get_state(self, t, eth_df):\n",
    "        num_rows = t - self.window_size + 1\n",
    "        if num_rows >= 0:\n",
    "            window = eth_df.iloc[num_rows : t + 1]\n",
    "        else:\n",
    "            repeated_first_row = pd.concat([pd.DataFrame(np.repeat(eth_df.iloc[[0]].values, -num_rows, axis=0), columns=eth_df.columns)])\n",
    "            new_data = eth_df.iloc[0 : t + 1]\n",
    "            window = pd.concat([repeated_first_row, new_data], ignore_index=True)  # prevents us from sampling data that doesn't exist at the start.\n",
    "        return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crypto_data(start_date, end_date, split_date):\n",
    "    try:\n",
    "        train = pd.read_csv(\"Datasets/train_eth.csv\", index_col=0)\n",
    "        test = pd.read_csv(\"Datasets/test_eth.csv\", index_col=0)\n",
    "    except:\n",
    "        df = HistoricalData('ETH-USD', 3600, start_date, end_date).retrieve_data()  # 3600 is the interval in seconds which is 1 hour\n",
    "        split_row = df.index.get_loc(split_date)\n",
    "        cols = df.columns.tolist()\n",
    "        df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=cols)\n",
    "        train = df[:split_row]\n",
    "        test = df[split_row:]\n",
    "        train.to_csv(\"Datasets/train_eth.csv\")\n",
    "        test.to_csv(\"Datasets/test_eth.csv\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crypto_data(start_date, end_date):\n",
    "    historic_data_df = HistoricalData('ETH-USD', 21600, start_date, end_date).retrieve_data()  # 3600 is the interval in seconds which is 1 hour\n",
    "    cols = historic_data_df.columns.tolist()\n",
    "    historic_data_df = pd.DataFrame(MinMaxScaler().fit_transform(historic_data_df), columns=cols)\n",
    "    historic_data_df.to_csv(\"Datasets/train_eth.csv\")\n",
    "    return historic_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-collected data\n",
    "def get_precollected_data(split_ratio=0.8):\n",
    "    # try:\n",
    "    historic_data = pd.read_csv(\"Datasets/eth_historical_data_simple.csv\", index_col=0)\n",
    "    print('Pre-collected data loaded successfully.')\n",
    "    # convert date to datetime if it isn't the index\n",
    "    if historic_data.index.name != 'Date':\n",
    "        historic_data['Date'] = pd.to_datetime(historic_data['Date'], infer_datetime_format=True)\n",
    "        print('Date converted to datetime.')\n",
    "        # set date as index\n",
    "        historic_data = historic_data.set_index('Date')\n",
    "        print('Date set as index.')\n",
    "    # drop nan values\n",
    "    historic_data = historic_data.dropna()\n",
    "    print('NaN values dropped.')\n",
    "    cols = historic_data.columns.tolist()\n",
    "    train_unscaled, test_unscaled = train_test_split(historic_data, train_size=split_ratio,shuffle=False)\n",
    "    train_scaled = pd.DataFrame(MinMaxScaler().fit_transform(train_unscaled), columns=cols)\n",
    "    test_scaled = pd.DataFrame(MinMaxScaler().fit_transform(test_unscaled), columns=cols)\n",
    "    print('Data split.')\n",
    "    # set the index to the date\n",
    "    train_unscaled = pd.DataFrame(train_unscaled,  columns=cols)\n",
    "    test_unscaled = pd.DataFrame(test_unscaled,  columns=cols)\n",
    "    # convert train_unscaled and test_unscaled to df with only the Close column\n",
    "    train_unscaled = train_unscaled[['Close']]\n",
    "    test_unscaled = test_unscaled[['Close']]\n",
    "\n",
    "    train_unscaled.to_csv(\"Output/train_unscaled.csv\")\n",
    "    train_scaled.to_csv(\"Output/train_scaled.csv\")\n",
    "    print('Train data saved.')\n",
    "    test_unscaled.to_csv(\"Output/test_unscaled.csv\")\n",
    "    test_scaled.to_csv(\"Output/test_scaled.csv\")\n",
    "    print('Test data saved.')\n",
    "    return train_scaled, test_scaled, train_unscaled, test_unscaled\n",
    "        \n",
    "    # except:\n",
    "    #     # throw error\n",
    "    #     print(\"No pre-collected data found. Please run the data collection script first.\")\n",
    "    #     exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data = pd.read_csv(\"Datasets/eth_historical_data_simple.csv\", index_col=0)\n",
    "\n",
    "historic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    INITIAL_INVESTMENT = 1000\n",
    "    NUM_EPISODES = 1000\n",
    "    START_DATE = \"2017-01-01-00-00\"\n",
    "    END_DATE = \"2020-01-01-00-00\"\n",
    "    TESTING_SPLIT = \"2022-01-01-00:00:00\" # formatting is different here because of how historic_crypto indexes dataframes\n",
    "\n",
    "    # train_df, test_df = get_crypto_data(START_DATE, END_DATE, TESTING_SPLIT)\n",
    "    train_df, test_df, train_close, test_close = get_precollected_data(split_ratio=0.8)\n",
    "    trading_agent = Agent(train_df.shape, NUM_EPISODES, window_size=48, gamma=0.95, update_interval=96, lr=0.01, min_epsilon=0.02)\n",
    "    \n",
    "    episode_mem = [{\"Actions\": [], \"Eth Held\": [], \"Cash Held\": [], \"Portfolio Value\": [], \"Reward\": [], \"Done\": [], \"Epsilon\": [], \"MSE Loss\": []} for i in range(NUM_EPISODES)]\n",
    "    t0 = time()\n",
    "\n",
    "    ######################## Training ########################\n",
    "    for s in range(NUM_EPISODES):\n",
    "        print(f\"\\n===== Episode {s + 1} / {NUM_EPISODES} =====\")\n",
    "        state = trading_agent.get_state(0, train_df)\n",
    "        cash_balance = INITIAL_INVESTMENT\n",
    "        portfolio_value_usd = INITIAL_INVESTMENT\n",
    "        # Reset the agent's portfolio at the beginning of each episode\n",
    "        trading_agent.portfolio = [0 , INITIAL_INVESTMENT, portfolio_value_usd]\n",
    "\n",
    "        done = False\n",
    "        for t in range(len(train_df) - 1):\n",
    "            if done:\n",
    "                break\n",
    "            action = trading_agent.get_action(state, cash_balance)\n",
    "            next_state = trading_agent.get_state(t + 1, train_df)\n",
    "            reward = trading_agent.trade(t, action, train_df, train_close, INITIAL_INVESTMENT, trading_fee_rate = 0.05)\n",
    "            eth_held, cash_held, new_portfolio_value = trading_agent.portfolio\n",
    "            cash_balance = cash_held  # update cash balance\n",
    "            portfolio_value_usd = new_portfolio_value  # update portfolio value\n",
    "            \n",
    "            if t != 0:  # if not the first trade\n",
    "                done = cash_balance <= 1 and eth_held <= 0.01\n",
    "            trading_agent.memory.add_exp(state, action, reward, next_state, done)\n",
    "            loss = trading_agent.train()\n",
    "            if not loss:\n",
    "                loss = 0\n",
    "            state = next_state\n",
    "            \n",
    "            episode_mem[s][\"Actions\"].append(int(action))\n",
    "            episode_mem[s][\"Eth Held\"].append(float(eth_held))\n",
    "            episode_mem[s][\"Cash Held\"].append(round(float(cash_balance), 2))\n",
    "            episode_mem[s][\"Portfolio Value\"].append(float(portfolio_value_usd))\n",
    "            episode_mem[s][\"Reward\"].append(float(reward))\n",
    "            episode_mem[s][\"Done\"].append(bool(done))\n",
    "            episode_mem[s][\"Epsilon\"].append(trading_agent.epsilon)\n",
    "            episode_mem[s][\"MSE Loss\"].append(float(loss))\n",
    "\n",
    "            if t % 100 == 0:\n",
    "                print(f\"Time step {t} / {len(train_df)}   |  Eth Held: {round(episode_mem[s]['Eth Held'][t], 7)}  |  Cash Held: {round(episode_mem[s]['Cash Held'][t], 2)}  |  Portfolio Value: {round(episode_mem[s]['Portfolio Value'][t], 3)}  |   MSE Loss: {round(episode_mem[s]['MSE Loss'][t], 3)}\")\n",
    "\n",
    "    with open('Output/training_scores.out', 'a') as f:\n",
    "            f.write(f\"EPISODE {s} (runtime: {time() - t0})   | Portfolio Value is {round(episode_mem[s]['Portfolio Value'][-1], 3)} Epsilon is {round(trading_agent.epsilon, 3)}   |   MSE Loss is {round(episode_mem[s]['MSE Loss'][-1], 3)}\\n\")\n",
    "\n",
    "    with open('Output/episode_mem.json', 'w') as f:\n",
    "        json.dump(episode_mem, f)\n",
    "\n",
    "    ######################## Testing ########################\n",
    "######################## Testing ########################\n",
    "    t0 = time()\n",
    "    testing_mem = {\"Actions\": [], \"Eth Held\": [], \"Cash Held\": [], \"Portfolio Value\": [], \"Reward\": [], \"Done\": []}\n",
    "    trading_agent.epsilon = 0\n",
    "    state = trading_agent.get_state(0, test_df)\n",
    "    cash_balance = INITIAL_INVESTMENT\n",
    "    portfolio_value_usd = INITIAL_INVESTMENT\n",
    "    # Reset the agent's portfolio at the beginning of each episode\n",
    "    trading_agent.portfolio = [0 , INITIAL_INVESTMENT, portfolio_value_usd]\n",
    "    # Reset the agent's portfolio at the beginning of each episode\n",
    "\n",
    "    done = False\n",
    "    for t in range(len(test_df) - 1):\n",
    "        if done:\n",
    "            break\n",
    "        action = trading_agent.get_action(state, cash_balance)\n",
    "        next_state = trading_agent.get_state(t + 1, test_df)\n",
    "        reward = trading_agent.trade(t, action, test_df, test_close, INITIAL_INVESTMENT, trading_fee_rate = 0.05)\n",
    "        eth_held, cash_held, new_portfolio_value  = trading_agent.portfolio\n",
    "        cash_balance = cash_held  # update cash balance\n",
    "        portfolio_value_usd = new_portfolio_value  # update portfolio value\n",
    "        if t != 0:  # if not the first trade\n",
    "                done = cash_balance <= 1 and eth_held <= 0.01\n",
    "        state = next_state\n",
    "\n",
    "        testing_mem[\"Actions\"].append(int(action))\n",
    "        testing_mem[\"Eth Held\"].append(float(eth_held))\n",
    "        testing_mem[\"Cash Held\"].append(round(float(cash_balance),2))\n",
    "        testing_mem[\"Portfolio Value\"].append(float(portfolio_value_usd))\n",
    "        testing_mem[\"Reward\"].append(float(reward))\n",
    "        testing_mem[\"Done\"].append(bool(done))\n",
    "\n",
    "        if t % 1 == 0:\n",
    "            print(f\"Time step {t} / {len(test_df)}   |   Eth Held: {round(testing_mem['Eth Held'][t], 7)}  |  Cash Held: {round(testing_mem['Cash Held'][t], 2)}  |  Portfolio Value: {round(testing_mem['Portfolio Value'][t], 3)}\")\n",
    "\n",
    "    with open('Output/testing_scores.out', 'a') as f:\n",
    "        f.write(f\"TESTING (runtime: {time() - t0})   |  Portfolio Value is {round(testing_mem['Portfolio Value'][-1], 3)}\\n\")\n",
    "\n",
    "    with open('Output/testing_mem.json', 'w') as f:\n",
    "        json.dump(testing_mem, f)\n",
    "\n",
    "    trading_agent.save_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df, test_df, train_close, test_close = get_precollected_data(split_ratio=0.8)\n",
    "\n",
    "def plot_data(data, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_df[\"Close\"], label=\"Ethereum Price\")\n",
    "    plt.plot(data[\"Portfolio Value\"], label=\"Portfolio Value\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "# visualize training data\n",
    "with open('Output/episode_mem.json', 'r') as f:\n",
    "    episode_mem = json.load(f)\n",
    "\n",
    "with open(\"Output/testing_mem.json\", \"r\") as f:\n",
    "    testing_mem = json.load(f)\n",
    "\n",
    "plot_data(episode_mem[29], \"Training Data\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot((testing_mem[\"Portfolio Value\"]))\n",
    "plt.plot(test_df['Close'])\n",
    "plt.legend([\"Ethereum Price\", \"Average Portfolio Value\"])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()\n",
    "# plot ethereum price history\n",
    "# set plot width and height\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(historic_data[\"Close\"])\n",
    "plt.title(\"Ethereum Price History\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()\n",
    "\n",
    "# # plot average portfolio value\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Portfolio Value\"])\n",
    "# plt.legend([\"Ethereum Price\", \"Average Portfolio Value\"])\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"Price\")\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Portfolio Value\"])\n",
    "# plt.title(\"Portfolio Value\")\n",
    "# plt.show()\n",
    "\n",
    "for i in range(NUM_EPISODES):\n",
    "    plt.plot(episode_mem[i][\"Realized Profit\"])\n",
    "plt.title(\"Realized Profit\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(episode_mem[0][\"Realized Profit\"])\n",
    "plt.title(\"Realized Profit\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(episode_mem[29][\"Realized Profit\"])\n",
    "plt.title(\"Realized Profit\")\n",
    "plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Reward\"])\n",
    "# plt.title(\"Reward\")\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"Epsilon\"])\n",
    "# plt.title(\"Epsilon\")\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(NUM_EPISODES):\n",
    "#     plt.plot(episode_mem[i][\"MSE Loss\"])\n",
    "# plt.title(\"MSE Loss\")\n",
    "# plt.show()\n",
    "\n",
    "# # visualize testing data\n",
    "# with open('testing_mem.json', 'r') as f:\n",
    "#     testing_mem = json.load(f)\n",
    "\n",
    "# # plot portfolio value vs ethereum price\n",
    "# plt.plot(testing_mem[\"Portfolio Value\"])\n",
    "# plt.title(\"Portfolio Value\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(testing_mem[\"Realized Profit\"])\n",
    "# plt.title(\"Realized Profit\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(testing_mem[\"Reward\"])\n",
    "# plt.title(\"Reward\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning_20220719",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23361560846584cf1ea895074b40a7ea878b24065d1c129dab67edcdc8abadf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
